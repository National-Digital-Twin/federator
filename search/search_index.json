{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>This repository contributes to the development of secure, scalable, and interoperable data-sharing infrastructure. It supports NDTP\u2019s mission to enable trusted, federated, and decentralised data-sharing across organisations.</p> <p>This repository is one of several open-source components that underpin NDTP\u2019s Integration Architecture (IA)\u2014a framework designed to allow organisations to manage and exchange data securely while maintaining control over their own information. The IA is actively deployed and tested across multiple sectors, ensuring its adaptability and alignment with real-world needs.</p> <p>For a complete overview of the Integration Architecture (IA) project, please see the Integration Architecture Documentation.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Java 21</li> <li>This repo uses a maven wrapper so no installation of maven is required.</li> <li>Docker</li> <li>Git</li> <li>Management-node</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Follow these steps to get started quickly with this repository. For detailed installation, configuration, and deployment, refer to the relevant MD files.</p>"},{"location":"#1-download-and-build","title":"1. Download and Build","text":"<p>To download from the github repository run the following commands:</p> <pre><code>git clone https://github.com/National-Digital-Twin/federator.git\ncd federator  \n</code></pre> <p>To run a demo with multiple Federator clients and multiple Federator servers run the following commands from the project root directory:</p> <p>Compile the java source code:  (Replace <code>./mvnw</code> with <code>mvn</code> to use the maven without the wrapper)</p> <pre><code>./mvnw clean install\n</code></pre> <p>Build the docker containers:</p> <pre><code>docker compose --file docker/docker-compose-multiple-clients-multiple-server.yml build\n</code></pre>"},{"location":"#2-run-build-version","title":"2. Run Build Version","text":"<p>Run the docker containers:</p> <pre><code>docker compose --file docker/docker-compose-multiple-clients-multiple-server.yml up\n</code></pre> <p>You should then see the service running within docker containers. These contain multiple clients and multiple servers and their supporting services. The service will move the data from the topic(s) in the kafka-src to federated topic(s) in kafka-target.</p>"},{"location":"#3-installation","title":"3. Installation","text":"<p>Refer to INSTALLATION.md for detailed installation steps, including required dependencies and setup configurations.</p>"},{"location":"#4-uninstallation","title":"4. Uninstallation","text":"<p>For steps to remove this repository and its dependencies, see UNINSTALL.md.</p>"},{"location":"#features","title":"Features","text":"<p>The federator enables secure data exchange between Integration Architecture nodes, supporting both server (producer) and client (consumer) roles. Key features include:</p> <ul> <li>Secure, scalable data sharing using Kafka as both source and target.</li> <li>Multiple federator servers and clients per organisation for flexible deployment.</li> <li>Filtering of Kafka messages for federation is based on the <code>securityLabel</code> in the Kafka message header and the client\u2019s credentials. The default filter performs an exact match between the client\u2019s credentials and the <code>securityLabel</code> header (e.g., <code>Security-Label:nationality=GBR</code>).</li> <li>Custom filtering logic can be configured; see Configuring a Custom Filter for details.</li> <li>Communication between federator servers and clients uses gRPC over mTLS for secure, authenticated data transfer.</li> <li>Federation currently supports RDF payloads, with extensibility hooks for other data formats on a per-topic basis.</li> <li>Integration with Management-Node for centralised configuration, topic management, and authorisation.</li> <li>Redis is used for offset tracking and short-lived configuration caching.</li> </ul> <p>An overview of the Federator service architecture is shown below:</p> <p> The diagram above shows how the Federator can be used to exchange data between Integration Architecture Nodes that are running within many different organisations. Each organisation could typically run many servers (producers) and many clients (consumers) to exchange data between their Integration Architecture Nodes.</p> <p>For example within the above diagram:</p> <ul> <li>Organisation 2 (Org 2) is shown to be running two servers, with one named \"Producer Node A1\" that is sending messages to the topic named \"DP1\"</li> <li>Organisation 1 (Org 1) is shown to be running a client called \"Consumer Node B2\" which is reading the messages from the topic named \"DP1\"</li> </ul> <p>It should be further noted that this diagram shows that many servers (or producers) and many clients (or consumers) can be configured within each organisation to exchange data between their Integration Architecture Nodes.</p> <p>Additional note on connectivity and security: - Multiple Federator Producers and Consumers can exchange data across organisations using gRPC over mTLS. - As long as they are configured to talk to the same Management-Node, they will obtain compatible configuration (topics, roles, filters, endpoints) required for their data exchange. - The Management-Node, together with the Identity Provider, issues the certificates/credentials and tokens that enable mutual TLS and authorisation. - This means any number of Producers and Consumers can safely share data so long as their exchange requirements are defined in, and served by, the Management-Node.</p>"},{"location":"#exchange-data-between-ia-nodes","title":"Exchange data between IA nodes","text":"<p>The Federator is designed to allow data exchange between Integration Architecture Nodes.  Kafka brokers are used as both a source of data and a target of data that is to be moved between Integration Architecture nodes. It is run in a distributed manner with multiple servers and clients.</p> <p>A simplistic view of the federator service is described below:</p>"},{"location":"#server-producer","title":"Server (Producer)","text":"<ol> <li>A server (producer) reads messages from a knowledge topic within the source Kafka broker.</li> <li>The server is configured so that it has a list of clients and the topics that they are allowed to read the messages from.</li> <li>The server also has a configurable filter that is used to decide if a message should be sent to a client.</li> <li>The server filters the messages based on the security label in the message header.</li> <li>The server streams the selected filtered messages to the client(s) using the gRPC protocol over a network.</li> </ol>"},{"location":"#client-consumer","title":"Client (Consumer)","text":"<ol> <li>A client (consumer) connects and then authenticates with its known server(s) using the gRPC protocol.</li> <li>A client requests the list of topics that it is allowed to read from the server.</li> <li>The client then requests the messages from the server for given topic(s).</li> <li>The client reads the messages and then writes them to a target Kafka broker to a topic name that is prefixed with 'federated'</li> </ol> <p>The underlying communication protocol is gRPC which is used to communicate between the server and client at the network level.</p>"},{"location":"#architecture","title":"Architecture","text":""},{"location":"#federator-server-producer","title":"Federator Server (Producer)","text":"<p>This app starts the data federation server that starts a gRPC service.</p> <p>This process contains the federator service supplying two RPC endpoints that are called by the client:</p> <ul> <li>Get Kafka Topics (obtain topics)</li> <li>Get kafka Consumer (consume topic)</li> </ul>"},{"location":"#obtain-topics","title":"Obtain Topics","text":"<ol> <li>Is passed a user request (a client-id and key)</li> <li>Authenticate the given credentials</li> <li>Returns the topics that have been assigned to the given user.</li> </ol>"},{"location":"#consume-topic","title":"Consume Topic","text":"<ol> <li>Is passed a topic request (client-id, key, topic &amp; offset)</li> <li>Validates the given details.</li> <li>Creates a message conductor to process the topic.</li> <li>Consumes and returns messages until stopped.</li> </ol>"},{"location":"#federator-client-consumer","title":"Federator Client (Consumer)","text":"<p>A somewhat simple app it does the following:</p> <ol> <li>Obtains topic(s) from the Server</li> <li>Checks with Redis to see what the offset is for given topic</li> <li>Obtain kafka consumer from the Server</li> <li>Process messages from consumer, adding to destination topic and update Redis offset count.</li> <li>Continue (4) until stopped.    If configured, it will repeat 1-5 upon failures</li> </ol> <p>Please refer to this context diagram as an overview of the federator service and its components:</p> <p></p> <p>This diagram illustrates the main components involved in a typical deployment: - Federator Producer and Federator Consumer communicating over gRPC (mTLS). - Kafka clusters used by producers and consumers. - Redis cache used for short\u2011lived configuration and offsets/tokens. - Management-Node service that provides configuration to Federators. - Identity Provider (e.g., Keycloak) used for authentication and authorisation. - Postgres databases used by the Management-Node and Identity Provider.</p> <p>See the Architecture section below for more detail on Producer and Consumer responsibilities.</p>"},{"location":"#testing-guide","title":"Testing Guide","text":""},{"location":"#running-unit-tests","title":"Running Unit Tests","text":"<p>Navigate to the root of the project and run <code>mvn test</code> to run the tests for the repository.</p>"},{"location":"#public-funding-acknowledgment","title":"Public Funding Acknowledgment","text":"<p>This repository has been developed with public funding as part of the National Digital Twin Programme (NDTP), a UK Government initiative. NDTP, alongside its partners, has invested in this work to advance open, secure, and reusable digital twin technologies for any organisation, whether from the public or private sector, irrespective of size.</p>"},{"location":"#license","title":"License","text":"<p>This repository contains both source code and documentation, which are covered by different licenses: - Code: Originally developed by Telicent UK Ltd, now maintained by National Digital Twin Programme. Licensed under the Apache License 2.0. - Documentation: Licensed under the Open Government Licence (OGL) v3.0.</p> <p>By contributing to this repository, you agree that your contributions will be licenced under these terms.</p> <p>See LICENSE, OGL_LICENSE, and NOTICE for details.</p>"},{"location":"#security-and-responsible-disclosure","title":"Security and Responsible Disclosure","text":"<p>We take security seriously. If you believe you have found a security vulnerability in this repository, please follow our responsible disclosure process outlined in SECURITY.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions that align with the Programme\u2019s objectives. Please read our Contributing guidelines before submitting pull requests.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This repository has benefited from collaboration with various organisations. For a list of acknowledgments, see ACKNOWLEDGMENTS.</p>"},{"location":"#support-and-contact","title":"Support and Contact","text":"<p>For questions or support, check our Issues or contact the NDTP team on ndtp@businessandtrade.gov.uk.</p> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity.</p>"},{"location":"authentication/","title":"Authentication Configuration","text":"<p>Repository: <code>federator</code> Description: <code>authentication configuration</code></p>"},{"location":"authentication/#federator-authentication-and-authorization-process","title":"Federator Authentication and Authorization Process","text":"<p>Federator uses mutual TLS (mTLS) and JWT-based authentication and authorization throughout all client-server and client-IDP interactions.</p>"},{"location":"authentication/#mtls-authentication","title":"mTLS Authentication","text":"<ul> <li>Client to IDP:</li> <li>When a client starts, it authenticates to the Identity Provider (IDP, e.g., Keycloak) using mTLS.</li> <li>The client presents its certificate to the IDP, which verifies the identity and issues a JWT token.</li> <li> <p>All properties for mTLS (keystore, truststore, passwords) are configured in <code>common-configuration.properties</code>.</p> </li> <li> <p>Client to Server (Producer):</p> </li> <li>The client establishes a secure connection to each producer/server using mTLS.</li> <li>Both client and server verify each other's certificates, ensuring mutual trust and secure communication.</li> </ul>"},{"location":"authentication/#jwt-token-acquisition-and-usage","title":"JWT Token Acquisition and Usage","text":"<ul> <li>After successful mTLS authentication with the IDP, the client receives a JWT token.</li> <li>The JWT token is attached to every request from the client to the management node and producer servers using an Auth Client Interceptor.</li> <li>The token contains claims that identify the client and its permissions.</li> </ul>"},{"location":"authentication/#server-side-authorization","title":"Server-Side Authorization","text":"<ul> <li>When a server receives a request, it validates the JWT token:</li> <li>Verifies the token signature using the IDP's JWKS endpoint.</li> <li>Checks the token's expiration and other standard claims.</li> <li>Authorization: The server checks the <code>aud</code> (audience) claim in the JWT to ensure the token is intended for this server or service. Only requests with valid audience claims are authorized to access protected resources.</li> <li>This process ensures that only authenticated and authorized clients can access server data and operations.</li> </ul>"},{"location":"authentication/#authentication-authorization-sequence-diagram","title":"Authentication &amp; Authorization Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant IDP\n    participant ManagementNode\n    participant ProducerServer\n\n    Client-&gt;&gt;IDP: Authenticate using mTLS, request JWT\n    IDP--&gt;&gt;Client: Return JWT token\n    Client-&gt;&gt;ManagementNode: Send request with JWT (Auth Client Interceptor)\n    ManagementNode-&gt;&gt;IDP: Validate JWT signature and claims\n    IDP--&gt;&gt;ManagementNode: Confirm JWT validity\n    ManagementNode-&gt;&gt;ManagementNode: Check 'aud' claim for authorization\n    alt Authorized\n        ManagementNode--&gt;&gt;Client: Grant access / return config\n    else Not authorized\n        ManagementNode--&gt;&gt;Client: Deny access\n    end\n    Client-&gt;&gt;ProducerServer: Send request with JWT (Auth Client Interceptor)\n    ProducerServer-&gt;&gt;IDP: Validate JWT signature and claims\n    IDP--&gt;&gt;ProducerServer: Confirm JWT validity\n    ProducerServer-&gt;&gt;ProducerServer: Check 'aud' claim for authorization\n    alt Authorized\n        ProducerServer--&gt;&gt;Client: Grant access / return data\n    else Not authorized\n        ProducerServer--&gt;&gt;Client: Deny access\n    end</code></pre>"},{"location":"authentication/#configuration-summary","title":"Configuration Summary","text":"<ul> <li>All mTLS and JWT properties are set in <code>common-configuration.properties</code>:</li> <li><code>idp.mtls.enabled</code>, <code>idp.keystore.path</code>, <code>idp.keystore.password</code>, <code>idp.truststore.path</code>, <code>idp.truststore.password</code>, <code>idp.jwks.url</code>, <code>idp.token.url</code>, <code>idp.client.id</code>, etc.</li> <li>The client does not use password-based authentication or connection-configuration.json for authentication anymore.</li> <li>All authentication and authorization is handled via mTLS and JWT tokens.</li> </ul>"},{"location":"authentication/#example-flow","title":"Example Flow","text":"<ol> <li>Client Startup:</li> <li>Loads configuration files and mTLS credentials.</li> <li>Authenticates to IDP using mTLS and requests JWT.</li> <li>JWT Token Usage:</li> <li>Attaches JWT to all requests to management node and producer servers.</li> <li>Server Validation:</li> <li>Validates JWT signature and claims, including <code>aud</code> for authorization.</li> <li>Allows or denies access based on token validity and audience.</li> </ol> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"client-configuration/","title":"Client Configuration","text":"<p>Repository: <code>federator</code> Description: <code>how to configure and run a federator client</code></p>"},{"location":"client-configuration/#overview","title":"Overview","text":"<p>This document describes the configuration properties for the Federator client, including connection settings, job parameters, caching, management node, dynamic configuration, and common configuration options shared across Federator components.</p>"},{"location":"client-configuration/#configuration-files","title":"Configuration Files","text":"<ul> <li>client.properties: Main client configuration. Location defined by the <code>FEDERATOR_CLIENT_PROPERTIES</code> environment variable.</li> <li>common-configuration.properties: Contains properties shared by both server and client. Location defined by the <code>common.configuration</code> property in <code>client.properties</code>.</li> </ul>"},{"location":"client-configuration/#client-properties","title":"Client Properties","text":"Property Description <code>kafka.sender.defaultKeySerializerClass</code> the default key serializer class <code>kafka.sender.defaultValueSerializerClass</code> the default value serializer class <code>kafka.bootstrapServers</code> the bootstrap server for the target Kafka instance <code>kafka.topic.prefix</code> the prefix for the target Kafka topics <code>kafka.consumerGroup</code> the consumer group for the client <code>kafka.pollDuration</code> duration to poll for messages (ms) - controls how long the client waits for messages before processing <code>kafka.pollRecords</code> number of records to poll per request - controls batching of messages for processing <code>kafka.additional.*</code> a repeatable optional property to set any additional kafka producer properties. These properties names must match with what kafka expects after the prefix has been removed <code>redis.host</code> the host for the redis cache <code>redis.port</code> the port for the redis cache <code>redis.tls.enabled</code> a flag to indicate if TLS is enabled for the redis cache <code>redis.username</code> the username for authenticating connections when redis Access Control List is being used. This can be left empty if either authentication is not required, or if redis only has <code>requirepass</code> enabled <code>redis.password</code> the password to be used for authenticating connections to redis. If either authentication is not required this can be left blank <code>redis.aes.key</code> if set, this will be used to encrypt values stored in Redis. The value must be Base64 and decode to 16, 24, or 32 bytes. <code>consumer.inactivity.timeout</code> duration of inactivity (ISO-8601, e.g. PT30S) before the client disconnects from the server due to inactivity <code>management.node.host</code> the hostname of the management node for client coordination and monitoring <code>management.node.port</code> the port of the management node for client coordination and monitoring"},{"location":"client-configuration/#running-the-client","title":"Running the Client","text":"<p>To run the Federator client, follow these steps:</p> <ol> <li>Prepare Configuration Files:</li> <li>Ensure <code>client.properties</code> contains all required client-specific settings.</li> <li>Ensure <code>common-configuration.properties</code> contains shared authentication and security properties.</li> <li> <p>Set the <code>common.configuration</code> property in <code>client.properties</code> to the path of your <code>common-configuration.properties</code> file.</p> </li> <li> <p>Set Environment Variables:</p> </li> <li><code>FEDERATOR_CLIENT_PROPERTIES</code>: Path to your <code>client.properties</code> file.</li> </ol> <p>Example (Linux/zsh):    <pre><code>export FEDERATOR_CLIENT_PROPERTIES=/path/to/client.properties\n</code></pre></p> <ol> <li>Run the Client:</li> <li>If using Java directly:      <pre><code>java -jar federator-client.jar\n</code></pre></li> <li> <p>If using Docker:      <pre><code>docker run --env FEDERATOR_CLIENT_PROPERTIES=/config/client.properties \\\n           -v /local/config:/config \\\n           federator-client:latest\n</code></pre></p> </li> <li> <p>Verify Startup:</p> </li> <li>Check logs for successful startup and connection to the management node.</li> <li>Ensure jobs are scheduled and connections to producers/servers are established as expected.</li> </ol> <p>Note: - All configuration properties must be set in the respective files. Properties cannot be overridden by environment variables. - The client will dynamically obtain server connection details from the management node as described above. - The location of <code>common-configuration.properties</code> is set by the <code>common.configuration</code> property in <code>client.properties</code>.</p>"},{"location":"client-configuration/#common-configuration-properties","title":"Common Configuration Properties","text":"<p>Common properties are defined in <code>common-configuration.properties</code> and may include:</p> Property Description <code>idp.mtls.enabled</code> Enable mutual TLS for Identity Provider (IDP) communication (<code>true</code>/<code>false</code>) <code>idp.client.secret</code> OAuth2 client secret (used when mTLS is disabled) <code>idp.jwks.url</code> JWKS URL of the IDP (used for verifying JWT signatures) <code>idp.token.url</code> Token endpoint URL of the IDP (used to fetch OAuth2 tokens) <code>idp.token.backoff</code> Backoff time in milliseconds before retrying a failed token request (default: 1000 ms) <code>idp.client.id</code> OAuth2 client ID (registered with the IDP) <code>idp.keystore.path</code> Path to client keystore file (PKCS12 or JKS) for mutual TLS <code>idp.keystore.password</code> Password for the client keystore <code>idp.truststore.path</code> Path to truststore containing the IDP's root CA or certificate chain <code>idp.truststore.password</code> Password for the truststore ... Other shared properties as required <p>These settings ensure secure authentication and authorisation between Federator components and the Identity Provider, supporting both mTLS and OAuth2 flows.</p>"},{"location":"client-configuration/#dynamic-consumer-configuration-via-management-node","title":"Dynamic Consumer Configuration via Management Node","text":"<p>Federator clients dynamically obtain their server connection details from the management node. The workflow is as follows:</p> <ol> <li>JWT Token Acquisition via mTLS:</li> <li>The client authenticates with the Identity Provider (IDP, e.g., Keycloak) using mutual TLS (mTLS) and obtains a JWT token.</li> <li> <p>Relevant properties for mTLS and OAuth2 are set in <code>common-configuration.properties</code> (see above).</p> </li> <li> <p>Management Node Authentication:</p> </li> <li>The client uses the JWT token to authenticate with the management node (configured via <code>management.node.host</code> and <code>management.node.port</code>).</li> <li> <p>The management node returns a JSON configuration describing available producers/servers and their products/topics.</p> </li> <li> <p>Dynamic Server Connection:</p> </li> <li>The client parses the returned JSON and connects to each producer/server listed, using the provided host, port, TLS, and topic details to fetch data.</li> </ol>"},{"location":"client-configuration/#example-management-node-response","title":"Example Management Node Response","text":"<pre><code>{\n    \"clientId\": \"FEDERATOR_HEG\",\n    \"producers\": [\n        {\n            \"products\": [\n                {\n                    \"name\": \"BrownfieldLandAvailability\",\n                    \"topic\": \"topic.BrownfieldLandAvailability\",\n                    \"consumers\": null\n                }\n            ],\n            \"name\": \"HEG-PRODUCER-1\",\n            \"description\": \"HEG Producer 1\",\n            \"active\": true,\n            \"host\": \"localhost\",\n            \"port\": 9001,\n            \"tls\": true,\n            \"idpClientId\": \"FEDERATOR_HEG\"\n        },\n        {\n            \"products\": [\n                {\n                    \"name\": \"PendingPlanningApplications\",\n                    \"topic\": \"topic.PendingPlanningApplications\",\n                    \"consumers\": null\n                }\n            ],\n            \"name\": \"BCC-PRODUCER-1\",\n            \"description\": \"BCC Producer 1\",\n            \"active\": true,\n            \"host\": \"localhost\",\n            \"port\": 9001,\n            \"tls\": true,\n            \"idpClientId\": \"FEDERATOR_BCC\"\n        }\n    ]\n}\n</code></pre> <p>Each producer in the response represents a server the client can connect to. The client uses the <code>host</code>, <code>port</code>, <code>tls</code>, and <code>idpClientId</code> fields to establish secure connections and fetch data from the specified topics.</p> <p>Required Properties: - <code>management.node.host</code>: Hostname of the management node. - <code>management.node.port</code>: Port of the management node. - mTLS and OAuth2 properties in <code>common-configuration.properties</code> for JWT acquisition.</p> <p>This approach allows clients to be dynamically configured and managed, supporting secure, scalable, and flexible data integration.</p>"},{"location":"client-configuration/#job-parameters-job-configuration","title":"Job Parameters &amp; Job Configuration","text":"<p>Job parameters control how the client processes messages and interacts with the server. Key properties include:</p> <ul> <li><code>kafka.pollDuration</code>: How long the client waits for messages before processing (in milliseconds).</li> <li><code>kafka.pollRecords</code>: Number of records to poll per request, controlling batching.</li> <li><code>consumer.inactivity.timeout</code>: Duration of inactivity before disconnecting from the server (ISO-8601 format, e.g. PT30S).</li> </ul> <p>These parameters can be tuned for performance and resource management.</p>"},{"location":"client-configuration/#job-scheduler","title":"Job Scheduler","text":"<p>Federator clients use a Job Scheduler to manage the lifecycle of jobs that control client connections and data fetching. The Job Scheduler supports:</p> <ul> <li>Add Jobs: Schedules new jobs (immediate or recurring) based on dynamic configuration from the management node. Each job may represent a connection to a producer/server and a data fetch operation for a specific topic.</li> <li>Delete Jobs: Removes scheduled jobs when they are no longer needed, causing the client to disconnect from the associated server or stop fetching data for a topic.</li> <li>Refresh Jobs: Synchronizes the set of active jobs with the latest configuration from the management node. This ensures the client only maintains connections and fetches data for currently relevant producers/servers and topics.</li> </ul> <p>The Job Scheduler enables real-time adaptation to changes in available producers/servers and their products/topics, supporting dynamic, scalable, and flexible client operation.</p>"},{"location":"client-configuration/#caching","title":"Caching","text":"<p>Federator clients can use Redis for caching. Relevant properties:</p> <ul> <li><code>redis.host</code>: Redis cache host.</li> <li><code>redis.port</code>: Redis cache port.</li> <li><code>redis.tls.enabled</code>: Enable TLS for Redis.</li> <li><code>redis.username</code>: Username for Redis ACL authentication.</li> <li><code>redis.password</code>: Password for Redis authentication.</li> <li><code>redis.aes.key</code>: Base64-encoded AES key for encrypting cached values (must decode to 16, 24, or 32 bytes).</li> </ul>"},{"location":"client-configuration/#management-node","title":"Management Node","text":"<p>Clients may connect to a management node for coordination, monitoring, and control. Relevant properties:</p> <ul> <li><code>management.node.host</code>: Hostname of the management node.</li> <li><code>management.node.port</code>: Port of the management node.</li> </ul>"},{"location":"client-configuration/#auth-client-interceptor","title":"Auth Client Interceptor","text":"<p>Federator clients use an Auth Client Interceptor to ensure secure, authenticated communication with servers (producers). The interceptor automatically attaches the JWT token\u2014obtained from the Identity Provider (IDP) using mTLS\u2014to every outgoing client request to servers. This mechanism:</p> <ul> <li>Ensures each request is authenticated and authorized according to the security policies of the management node and producers.</li> <li>Integrates seamlessly with dynamic configuration and the job scheduler, so all connections established as a result of job scheduling or configuration updates are properly secured.</li> <li>Relies on the mTLS and OAuth2 properties in <code>common-configuration.properties</code> for token acquisition and management.</li> </ul> <p>No additional configuration is required for the interceptor beyond the standard authentication properties. This approach guarantees that all client-server interactions are protected and compliant with Federator's security requirements.</p>"},{"location":"client-configuration/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Client\n    participant ConfigFiles\n    participant IDP\n    participant ManagementNode\n    participant JobScheduler\n    participant ProducerServer\n\n    User-&gt;&gt;Client: Start client process\n    Client-&gt;&gt;ConfigFiles: Load client.properties\n    Client-&gt;&gt;ConfigFiles: Load common-configuration.properties (via common.configuration)\n    Client-&gt;&gt;IDP: Authenticate using mTLS, request JWT\n    IDP--&gt;&gt;Client: Return JWT token\n    Client-&gt;&gt;ManagementNode: Authenticate with JWT, request dynamic config\n    ManagementNode--&gt;&gt;Client: Return producer/server config JSON\n    Client-&gt;&gt;JobScheduler: Schedule jobs for each producer/topic\n    loop For each job\n        JobScheduler-&gt;&gt;ProducerServer: Establish connection, fetch data\n        ProducerServer--&gt;&gt;JobScheduler: Return data\n    end\n    JobScheduler-&gt;&gt;Client: Update job status, manage connections</code></pre>"},{"location":"client-configuration/#c4-diagram","title":"C4 Diagram","text":"<p>A C4 diagram provides a high-level architectural view of the Federator client and its interactions.</p> <pre><code>%%{init: { 'theme': 'default' }}%%\nC4Container\n\nPerson(user, \"User\", \"Initiates and monitors client operation\")\nSystem(client, \"Federator Client\", \"Main client application\")\nContainer(config, \"Configuration Files\", \"Properties Files\", \"client.properties, common-configuration.properties\")\nContainer(idp, \"Identity Provider (IDP)\", \"OAuth2/Keycloak\", \"Issues JWT tokens via mTLS\")\nContainer(mgmt, \"Management Node\", \"Federator Management Node\", \"Provides dynamic configuration and job info\")\nContainer(job, \"Job Scheduler\", \"JobRunr/Quartz\", \"Schedules, refreshes, and deletes jobs\")\nContainer(producer, \"Producer Server\", \"Kafka/gRPC Server\", \"Provides data streams/topics\")\nContainer(redis, \"Redis Cache\", \"Redis\", \"Caching for client operations\")\n\nRel(user, client, \"Starts, configures, and monitors\")\nRel(client, config, \"Loads configuration\")\nRel(client, idp, \"Authenticates via mTLS, requests JWT\")\nRel(client, mgmt, \"Authenticates with JWT, requests dynamic config\")\nRel(mgmt, client, \"Returns producer/server config JSON\")\nRel(client, job, \"Schedules jobs for each producer/topic\")\nRel(job, producer, \"Connects, fetches data\")\nRel(client, redis, \"Uses for caching\")\n\nRel(producer, job, \"Returns data\")\nRel(job, client, \"Updates job status\")</code></pre> <p>You can view and edit this diagram using Mermaid live editors or compatible Markdown viewers.</p> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"deployment/","title":"Deploying a Federator Locally","text":"<p>Repository: <code>federator</code> Description: <code>deploying the federator locally.</code></p>"},{"location":"deployment/#a-guide-showing-the-steps-on-deploying-a-federator-locally","title":"A Guide showing the steps on deploying a federator locally.","text":""},{"location":"deployment/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Federator Deployment</li> <li>Changing Topics (Optional)</li> <li>Additional Test Data (Optional)</li> </ul>"},{"location":"deployment/#introduction","title":"Introduction","text":"<p>The following guide shows how to deploy a federator locally that can be connected to two IA Nodes.</p> <p>This guide will assume you have already followed the setup in Running the federator locally and have already set up your maven <code>.m2/settings.xml</code> profile and have a working PAT (Personal Access Token).</p>"},{"location":"deployment/#federator-deployment","title":"Federator Deployment","text":"<ol> <li> <p>Locate the <code>docker/docker-compose-grpc.yml</code> file which contains a federator    client and server.</p> </li> <li> <p>Replace the <code>image</code> of the <code>federator-server</code> and <code>federator-client</code> with their respective GHCR images, we recommend    checking the released federator packages    for an up-to-date version.</p> </li> </ol> <p><pre><code> federator-server:\n #    image: uk.gov.dbt.ndtp/${ARTIFACT_ID}-server:${VERSION}\n     image: ghcr.io/national-digital-twin/federator/federator-server:0.90.0\n\n... Rest of file ...\n\nfederator-client:\n#  image: uk.gov.dbt.ndtp/${ARTIFACT_ID}-client:${VERSION}\n   image: ghcr.io/national-digital-twin/federator/federator-client:0.90.0\n</code></pre> 3. Start the docker containers with the following command:</p> <p><pre><code>docker compose --file docker/docker-compose-grpc.yml up -d\n</code></pre> 4. Wait for the services to start and data to be loaded into the federated Kafka topic. 5. Check federated messages have been filtered in Kafka with the following Kafka consumer command:</p> <pre><code>./kafka-console-consumer.sh --bootstrap-server localhost:29093 --topic federated-client1-FederatorServer1-knowledge --from-beginning\n</code></pre>"},{"location":"deployment/#changing-topics-optional","title":"Changing Topics (Optional)","text":"<p>This sections shows how to add another topic to the federator 'stack' to choose where data is sent and then federated.</p> <ol> <li> <p>Locate the <code>docker/docker-grpc-resources/docker-compose-shared.yml</code> file which contains the source    and target kafka topics, and some helpful shell scripts to add and remove data.</p> </li> <li> <p>On line 135, add <code>RDF</code> to the end:</p> </li> </ol> <pre><code>environment:\n  BOOTSTRAP_VALUE: \"kafka-src:19092\"\n  KAFKA_TOPICS: \"knowledge knowledge1 knowledge2 RDF\"\n</code></pre>"},{"location":"deployment/#additional-test-data-optional","title":"Additional Test Data (Optional)","text":"<p>This sections shows how to add additional test data to a new topic that can be monitored by the federator.</p> <ol> <li> <p>Like changing topics Locate the <code>docker/docker-grpc-resources/docker-compose-shared.yml</code> file</p> </li> <li> <p>Update line 152 by adding the <code>RDF</code> Topic and <code>${KNOWLEDGE_DATA_3}</code>:</p> </li> </ol> <p><pre><code>environment:\n KAFKA_BROKER_SERVER : \"kafka-src:19092\"\n KNOWLEDGE_TOPIC: \"knowledge knowledge1 knowledge2 RDF\"\n KNOWLEDGE_DATA: \"${KNOWLEDGE_DATA} ${KNOWLEDGE_DATA_1} ${KNOWLEDGE_DATA_2} ${KNOWLEDGE_DATA_3}\"\n</code></pre> 3. Update <code>.env</code> file located in <code>/docker/.env</code> with the following sample data:</p> <p><pre><code>KNOWLEDGE_DATA_3=simple-sample-test3.dat\n</code></pre> 4. Create a new <code>simple-sample-test3.dat</code> file containing triples data and an optional <code>Security-Label</code> header    in <code>docker/input</code> (or copy <code>simple-sample-test2.dat</code> and rename the file). 5. Start your containers with the following docker compose command:</p> <p><pre><code>docker compose --file docker/docker-compose-grpc.yml up -d\n</code></pre> 6. Check the federated messages on your new topic:</p> <pre><code>./kafka-console-consumer.sh --bootstrap-server localhost:29093 --topic federated-client1-FederatorServer1-RDF --from-beginning\n</code></pre> <p>or your un-federated messages on the source kafka topic:</p> <pre><code>./kafka-console-consumer.sh --bootstrap-server localhost:19093 --topic RDF --from-beginning\n</code></pre> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"logging-configuration/","title":"Logging configuration","text":"<p>Repository: <code>federator</code> Description: <code>how to change the logging configuration</code></p>"},{"location":"logging-configuration/#overview","title":"Overview","text":"<p>This describes how to change the logging level of the application.</p>"},{"location":"logging-configuration/#changing-the-level-of-detail-in-the-logging-configuration","title":"Changing the level of detail in the logging configuration","text":"<p>The logging configuration is defined in a properties file <code>logback.xml</code> which is located in the <code>src/main/resources</code> directory.</p> <p>The logging configuration is defined in the <code>&lt;root level=\"Logging-Level\"&gt;</code> element of the file. This element defines the root logger and is the parent of all other loggers in the configuration.</p> <p>For example to set the level of logging to debug, the root element configuration would look like this:</p> <pre><code>&lt;root level=\"DEBUG\"&gt;\n    &lt;appender-ref ref=\"ASYNC_CONSOLE\"/&gt;\n&lt;/root&gt;\n</code></pre> <p>The level attribute can be set to one of the following values: <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, <code>OFF</code></p> <p>Rebuild the application jar and docker images to apply the changes.</p> <p>Once completed you can confirm the changes by running the application and checking the logs (unless you have set them to <code>OFF</code>):</p> <pre><code>11:35:25,960 |-INFO in ch.qos.logback.classic.model.processor.RootLoggerModelHandler - Setting level of ROOT logger to ERROR\n</code></pre> <p>For more information please refer to the logback documentation</p> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"running-locally/","title":"Running the Federator locally","text":"<p>Repository: <code>federator</code> Description: <code>running the federator locally</code></p>"},{"location":"running-locally/#outlines-how-to-install-build-and-run-the-federator-client-and-server-locally","title":"Outlines how to install, build, and run the Federator client and server locally.","text":""},{"location":"running-locally/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Requirements</li> <li>Configuration</li> <li>Maven setup</li> <li>Compiling and Running</li> <li>Viewing the Kafka Topics</li> <li>Further Examples</li> </ul>"},{"location":"running-locally/#introduction","title":"Introduction","text":"<p>The following outlines how to take the java code and run the application locally using resources that are run within Docker containers.  These docker containers will start up the Kafka, Zookeeper, and Redis resources.</p> <p>For more information on how to run the docker containers see the docker readme.</p>"},{"location":"running-locally/#requirements","title":"Requirements","text":"<p>This will require: Java (version 21+), Docker, and Git to be installed. Docker desktop is the preferred tool, however a vanilla Docker installation will also work. If you are an Ubuntu user then you might also need to follow some extra steps to get Docker Desktop working see the link here.</p>"},{"location":"running-locally/#configuration","title":"Configuration","text":"<p>Please see the following links for configuration details: - server configuration for more details. - client configuration for more details. - authentication configuration for more details. - logging configuration for more details.</p>"},{"location":"running-locally/#maven-setup","title":"Maven setup","text":"<p>Currently, we are using GitHub package repositories to distribute libraries. To ensure that Maven have access to the package repositories we need to update the settings. Instructions on how to do this can be found here which will contain additional information.</p> <p>You will need to generate a personal access token that has access to at least reading package registries. Open up <code>$MAVEN_HOME/.m2/settings.xml</code> and add the following</p> <pre><code>&lt;settings&gt;\n    &lt;activeProfiles&gt;\n        &lt;activeProfile&gt;github&lt;/activeProfile&gt;\n    &lt;/activeProfiles&gt;\n\n    &lt;!--  Optional as this will also be defined in the pom file  --&gt;\n    &lt;profiles&gt;\n        &lt;profile&gt;\n            &lt;id&gt;github&lt;/id&gt;\n            &lt;repositories&gt;\n                &lt;repository&gt;\n                    &lt;id&gt;central&lt;/id&gt;\n                    &lt;url&gt;https://repo1.maven.org/maven2&lt;/url&gt;\n                &lt;/repository&gt;\n                &lt;repository&gt;\n                    &lt;id&gt;github&lt;/id&gt;\n                    &lt;url&gt;https://maven.pkg.github.com/National-Digital-Twin/*&lt;/url&gt;\n                    &lt;snapshots&gt;\n                        &lt;enabled&gt;true&lt;/enabled&gt;\n                    &lt;/snapshots&gt;\n                &lt;/repository&gt;\n            &lt;/repositories&gt;\n        &lt;/profile&gt;\n    &lt;/profiles&gt;\n\n    &lt;servers&gt;\n        &lt;server&gt;\n            &lt;id&gt;github&lt;/id&gt;\n            &lt;username&gt;YOUR_GITHUB_USERNAME&lt;/username&gt;\n            &lt;password&gt;YOUR_TOKEN&lt;/password&gt;\n        &lt;/server&gt;\n    &lt;/servers&gt;\n&lt;/settings&gt;\n</code></pre>"},{"location":"running-locally/#compiling-and-running","title":"Compiling and Running","text":""},{"location":"running-locally/#compile-and-build-the-jar-file","title":"Compile and build the JAR file","text":"<ul> <li>From a terminal window navigate to the root directory of the project.  Run the following command to compile the code.</li> </ul> <pre><code>./mvnw clean install\n</code></pre> <p>You should now have built both the server and client Jar files.</p>"},{"location":"running-locally/#running-docker-compose","title":"Running Docker Compose","text":"<p>To start the federators common docker services run the following command:</p> <pre><code>docker compose --file docker/docker-grpc-resources/docker-compose-shared.yml up -d\n</code></pre> <ul> <li>This will create the kafka source <code>kafka-src</code> and target <code>kafka-target</code> together with two zookeeper and one redis containers.</li> <li>It will also create four additional containers to create the test topics and populate them with test data.</li> </ul> <p>To confirm that the containers are running, check this guide:</p>"},{"location":"running-locally/#running-the-federator-server-code","title":"Running the Federator Server Code","text":"<p>Start the federation server code. Note the parameters that are being passed based on properties to tell it the access topics, the Kafka server instance location and port. The remaining properties are picked up from the default properties file bundled into the Jar file.</p> <ul> <li>Export the properties file location to the environment variable <code>FEDERATOR_SERVER_PROPERTIES</code></li> </ul> <pre><code>export FEDERATOR_SERVER_PROPERTIES=./src/configs/server.properties\n</code></pre> <p>This will configure the server to use the properties file server.properties </p> <ul> <li>Start the server code contained in a jar file:</li> </ul> <pre><code>java -jar ./target/federator-server-0.3.0.jar\n</code></pre>"},{"location":"running-locally/#running-the-federator-client-code","title":"Running the Federator Client Code","text":"<ul> <li>Export the properties file location to the environment variable <code>FEDERATOR_CLIENT_PROPERTIES</code></li> </ul> <pre><code>export FEDERATOR_CLIENT_PROPERTIES=./src/configs/client.properties \n</code></pre> <p>This will set the client to use the properties file client.properties together with the related connection-configuration.json file.</p> <ul> <li>Start the client code contained in a jar file:</li> </ul> <pre><code>java -jar ./target/federator-client-0.3.0.jar\n</code></pre>"},{"location":"running-locally/#stopping-the-client-and-server-code","title":"Stopping the Client and Server Code","text":"<ul> <li>To stop either of the client or server code use <code>ctrl c</code> in the terminal window where the code is running.</li> </ul>"},{"location":"running-locally/#stopping-the-docker-containers","title":"Stopping the Docker Containers","text":"<ul> <li>Stop the docker containers with the commands in the docker docs</li> </ul>"},{"location":"running-locally/#viewing-the-kafka-topics","title":"Viewing the Kafka Topics","text":""},{"location":"running-locally/#viewing-the-kafka-topics-on-the-target-kafka-instance","title":"Viewing the Kafka Topics on the Target Kafka Instance","text":"<ul> <li>To view the processed Kafka topics on the target Kafka instance use the following command:</li> </ul> <pre><code>docker exec -it kafka-target kafka-console-consumer --bootstrap-server localhost:29093 --topic federated-Localhost-createDataTopic1 --from-beginning --property print.headers=true\n</code></pre> <ul> <li>This will show the messages that have been processed by the federator client code.</li> <li>The topic name will be prefixed with <code>federated-</code> and the original topic name (<code>federated-Localhost-createDataTopic1</code>).</li> </ul>"},{"location":"running-locally/#viewing-the-kafka-topics-on-the-source-kafka-instance","title":"Viewing the Kafka Topics on the Source Kafka Instance","text":"<ul> <li>To view the Kafka topics on the source Kafka instance use the following command:</li> </ul> <pre><code>docker exec -it kafka-src kafka-console-consumer --bootstrap-server localhost:19093 --topic createDataTopic1 --from-beginning --property print.headers=true\n</code></pre> <ul> <li>This will show the test messages that have been loaded into the source Kafka instance.</li> </ul>"},{"location":"running-locally/#smoke-tests","title":"Smoke Tests","text":"<ul> <li>To run smoke tests, you will need to set up the compose files in this guide</li> </ul>"},{"location":"running-locally/#further-examples","title":"Further Examples","text":"<p>Further example configurations can be found in the configs readme.</p> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"server-configuration/","title":"Server Configuration","text":"<p>Repository: <code>federator</code> Description: <code>server configuration</code></p>"},{"location":"server-configuration/#overview","title":"Overview","text":"<p>This document describes the configuration properties for the Federator server, including connection settings, filtering, and common configuration options shared across Federator components.</p>"},{"location":"server-configuration/#configuration-files","title":"Configuration Files","text":"<ul> <li>server.properties: Main server configuration. Location defined by the <code>FEDERATOR_SERVER_PROPERTIES</code> environment variable.</li> <li>common-configuration.properties: Contains properties shared by both server and client. Location defined by the <code>FEDERATOR_COMMON_PROPERTIES</code> environment variable.</li> </ul>"},{"location":"server-configuration/#server-properties","title":"Server Properties","text":"Property Description <code>kafka.bootstrapServers</code> Bootstrap servers for the source Kafka instance <code>kafka.defaultKeyDeserializerClass</code> Default key deserializer class <code>kafka.defaultValueDeserializerClass</code> Default value deserializer class <code>kafka.consumerGroup</code> Consumer group for the server <code>kafka.pollDuration</code> Duration to poll for messages (ms) <code>kafka.pollRecords</code> Number of records to poll per request <code>kafka.additional.*</code> Optional: Additional Kafka consumer properties (prefix removed before passing to Kafka) <code>shared.headers</code> Definition of Kafka message headers <code>filter.shareAll</code> If true, all messages are shared (no filtering) <code>server.port</code> Port the server listens on <code>server.tlsEnabled</code> If true, enables TLS for gRPC communication <code>server.keepAliveTime</code> Keep-alive time for the server (ms) <code>consumer.inactivity.timeout</code> Duration of inactivity (ISO-8601, e.g. PT30S) before the server disconnects the consumer <p>Note: - The <code>consumer.inactivity.timeout</code> property controls how long the server will wait for messages before disconnecting a consumer due to inactivity. If no messages are received within this duration, the consumer connection is closed automatically. This helps free up resources and ensures efficient operation.</p>"},{"location":"server-configuration/#common-configuration-properties","title":"Common Configuration Properties","text":"<p>Common properties are defined in <code>common-configuration.properties</code> and may include:</p> Property Description <code>idp.mtls.enabled</code> Enable mutual TLS for Identity Provider (IDP) communication (<code>true</code>/<code>false</code>) <code>idp.client.secret</code> OAuth2 client secret (used when mTLS is disabled) <code>idp.jwks.url</code> JWKS URL of the IDP (used for verifying JWT signatures) <code>idp.token.url</code> Token endpoint URL of the IDP (used to fetch OAuth2 tokens) <code>idp.token.backoff</code> Backoff time in milliseconds before retrying a failed token request (default: 1000 ms) <code>idp.client.id</code> OAuth2 client ID (registered with the IDP) <code>idp.keystore.path</code> Path to client keystore file (PKCS12 or JKS) for mutual TLS <code>idp.keystore.password</code> Password for the client keystore <code>idp.truststore.path</code> Path to truststore containing the IDP's root CA or certificate chain <code>idp.truststore.password</code> Password for the truststore ... Other shared properties as required <p>All properties can be set directly or via environment variables (see <code>${...}</code> syntax in the file). These settings ensure secure authentication and authorisation between Federator components and the Identity Provider, supporting both mTLS and OAuth2 flows.</p>"},{"location":"server-configuration/#custom-filter-configuration","title":"Custom Filter Configuration","text":"<p>Federator supports custom filtering logic for Kafka messages. The producer (server) receives consumer attribute configuration from the Management-Node as JSON, for example:</p> <pre><code>{\n  \"clientId\": \"FEDERATOR_HEG\",\n  \"producers\": [\n    {\n      \"products\": [\n        {\n          \"name\": \"BrownfieldLandAvailability\",\n          \"topic\": \"topic.BrownfieldLandAvailability\",\n          \"consumers\": [\n            {\n              \"name\": \"ENV-CONSUMER-1\",\n              \"idpClientId\": \"FEDERATOR_ENV\",\n              \"attributes\": [\n                { \"name\": \"nationality\", \"value\": \"GBR\", \"type\": \"string\" },\n                { \"name\": \"clearance\", \"value\": \"0\", \"type\": \"string\" },\n                { \"name\": \"organisation_type\", \"value\": \"NON-GOV3\", \"type\": \"string\" }\n              ]\n            }\n          ]\n        }\n      ],\n      \"name\": \"HEG-PRODUCER-1\",\n      \"idpClientId\": \"FEDERATOR_HEG\"\n    }\n  ]\n}\n</code></pre>"},{"location":"server-configuration/#filtering-logic","title":"Filtering Logic","text":"<ul> <li>No attributes set: All data is passed to the client.</li> <li>One attribute set: Only messages with a matching attribute and value are passed.</li> <li>Multiple attributes set: Messages must match all attributes (AND logic).</li> </ul> <p>The filter checks Kafka message headers for the required attributes and values. For example, if a consumer requires <code>nationality=GBR</code> and <code>clearance=0</code>, only messages with both headers will be delivered.</p>"},{"location":"server-configuration/#example-filter-implementation","title":"Example Filter Implementation","text":"<pre><code>public class AttributeBasedMessageFilter implements MessageFilter {\n    private final Map&lt;String, String&gt; requiredAttributes;\n    public AttributeBasedMessageFilter(Map&lt;String, String&gt; requiredAttributes) {\n        this.requiredAttributes = requiredAttributes;\n    }\n    @Override\n    public boolean filterOut(KafkaEvent&lt;?, ?&gt; message) {\n        if (requiredAttributes == null || requiredAttributes.isEmpty()) {\n            return false; // No attributes: pass all messages\n        }\n        for (Map.Entry&lt;String, String&gt; entry : requiredAttributes.entrySet()) {\n            String headerValue = message.getHeader(entry.getKey());\n            if (!entry.getValue().equals(headerValue)) {\n                return true; // Filter out if any attribute does not match\n            }\n        }\n        return false; // Pass if all attributes match\n    }\n}\n</code></pre> <p>For details on authentication and key generation, see Authentication Configuration.</p>"},{"location":"server-configuration/#interceptors","title":"Interceptors","text":"<p>Federator uses gRPC server interceptors to enforce authentication and authorization for all incoming requests:</p>"},{"location":"server-configuration/#authserverinterceptor","title":"AuthServerInterceptor","text":"<ul> <li>Validates the presence and format of the Authorization header (expects a Bearer token).</li> <li>Verifies the JWT token using the Identity Provider (IDP) public keys or mTLS.</li> <li>Rejects requests with missing, invalid, or expired tokens.</li> <li>Configuration properties used: <code>idp.jwks.url</code>, <code>idp.token.url</code>, <code>idp.mtls.enabled</code>, etc.</li> </ul>"},{"location":"server-configuration/#consumerverificationserverinterceptor","title":"ConsumerVerificationServerInterceptor","text":"<ul> <li>Extracts the consumer ID from the JWT token.</li> <li>Validates the token audience against the configured client ID (<code>idp.client.id</code>).</li> <li>Authorizes the consumer by checking the producer configuration (from Management-Node) to ensure the consumer is permitted for the requested product/topic.</li> <li>Attaches the verified consumer ID to the gRPC context for downstream use.</li> <li>Rejects requests from unauthorized consumers or with invalid audience.</li> </ul> <p>These interceptors are essential for secure, multi-tenant operation, ensuring only authenticated and authorized clients can access federated data. For implementation details, see: - <code>AuthServerInterceptor.java</code> - <code>ConsumerVerificationServerInterceptor.java</code></p>"},{"location":"server-configuration/#grpc-request-sequence-diagram","title":"gRPC Request Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant Client as gRPC Client\n    participant Auth as AuthServerInterceptor\n    participant Verify as ConsumerVerificationServerInterceptor\n    participant Server as Federator Server\n    participant Mgmt as Management-Node\n    participant Kafka as Kafka/Redis\n    participant Filter as Custom Filter\n\n    Client-&gt;&gt;Auth: Send gRPC request with Bearer token\n    Auth-&gt;&gt;Auth: Validate Authorization header &amp; JWT\n    Auth--&gt;&gt;Client: Reject if invalid\n    Auth-&gt;&gt;Verify: Pass request if valid\n    Verify-&gt;&gt;Verify: Extract consumer ID from JWT\n    Verify-&gt;&gt;Verify: Validate audience (idp.client.id)\n    Verify-&gt;&gt;Mgmt: Fetch producer/consumer config\n    Verify-&gt;&gt;Verify: Authorize consumer for topic\n    Verify--&gt;&gt;Client: Reject if unauthorized\n    Verify-&gt;&gt;Server: Pass request if authorized\n    Server-&gt;&gt;Kafka: Read messages from Kafka\n    Server-&gt;&gt;Filter: Apply custom filter (attributes)\n    Filter--&gt;&gt;Server: Return filtered messages\n    Server--&gt;&gt;Client: Send federated data</code></pre> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"smoke-tests/one-to-one/","title":"One to One (Single Client with Single Server) Smoke Tests","text":""},{"location":"smoke-tests/one-to-one/#use-case","title":"Use Case","text":"<p>Given I send the <code>simple-sample-test.dat</code> data file to producer Kafka</p> <p>When that file contains 40 records of test data and the following filters are applied:</p> <ul> <li>Nationality: <code>GBR</code></li> <li>Clearance: <code>O</code></li> <li>Organisation Type: <code>NON-GOV</code></li> </ul> <p>Then I should see 23 records in the consumer IA Node data store, and those records should contain the same attributes listed above (Nationality, Clearance, Organisation Type)</p>"},{"location":"smoke-tests/one-to-one/#running-the-tests","title":"Running the tests","text":"<p>To run the tests you'll need to spin up the following docker compose file:</p> <pre><code>docker compose --file docker/docker-compose-grpc.yml up -d   \n</code></pre>"},{"location":"smoke-tests/one-to-one/#changing-the-test-parameters","title":"Changing the test parameters","text":""},{"location":"smoke-tests/one-to-one/#changing-the-test-data-file","title":"Changing the test data file","text":"<p>To change the test data file that is sent via kafka, you must modify the <code>KNOWLEDGE_DATA</code> environment variable in the <code>docker/.env</code> to the filename of your desired file, ensuring that the file is in the <code>docker/input/knowledge</code> directory.</p> <pre><code>KNOWLEDGE_DATA=simple-sample-test.dat  # Change this to your desired filename\n</code></pre>"},{"location":"smoke-tests/one-to-one/#changing-the-filters","title":"Changing the filters","text":"<p>Changing the data filters can be done by modifiying the <code>attributes</code> in <code>docker/docker-grpc-resources/access.json</code></p> <pre><code>\"attributes\": {\n    \"nationality\": \"GBR\",\n    \"clearance\": \"O\",\n    \"organisation_type\": \"NON-GOV\"\n}\n</code></pre>"},{"location":"smoke-tests/one-to-one/#viewing-the-test-outcome","title":"Viewing the test outcome","text":"<p>The test outcome can be viewed by checking the logs of kafka-message-counter, which will check the message count in the Kafka topic against the expected number of messages:</p> <pre><code>docker logs kafka-message-counter --follow \n</code></pre> <p>There may be cases where the message counter needs more time to pick up the federated messages. To resolve any test failures related to this, you can restart the <code>kafka-message-counter</code> container</p> <pre><code>docker restart kafka-message-counter\n</code></pre> <p>and then rerun the logs command above.</p>"},{"location":"smoke-tests/one-to-one/#clean-up","title":"Clean up","text":"<p>When finished with the smoke tests you can take down the containers to save resources:</p> <pre><code>docker compose --file docker/docker-compose-grpc.yml down\n</code></pre>"}]}