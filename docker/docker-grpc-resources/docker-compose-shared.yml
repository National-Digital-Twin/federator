# SPDX-License-Identifier: Apache-2.0
# Originally developed by Telicent Ltd.; subsequently adapted, enhanced, and maintained by the National Digital Twin Programme.

# Copyright (c) Telicent Ltd.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Modifications made by the National Digital Twin Programme (NDTP)
# © Crown Copyright 2025. This work has been developed by the National Digital Twin Programme
# and is legally attributed to the Department for Business and Trade (UK) as the governing entity.

services:
  zookeeper-src:
    image: confluentinc/cp-zookeeper:7.5.3
    container_name: zookeeper-src
    environment:
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zookeeper-src:2888:3888
    networks:
      - core
    ports:
      - 32181:32181
    hostname: zookeeper-src
    restart: on-failure

  kafka-src:
    image: confluentinc/cp-kafka:7.5.3
    container_name: kafka-src
    environment:
      KAFKA_ADVERTISED_LISTENERS: EXTERNAL_DOCKER_INTERNAL://localhost:19093,LISTENER_DOCKER_INTERNAL://kafka-src:19092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper-src:32181"
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
    ports:
      - "19093:19093"
    healthcheck:
      test: "kafka-topics --list --bootstrap-server localhost:9092 || exit 1"
      interval: 1s
      timeout: 60s
      retries: 60
    networks:
      - core
    depends_on:
      - zookeeper-src
    restart: on-failure

  zookeeper-target:
    image: confluentinc/cp-zookeeper:7.5.3
    container_name: zookeeper-target
    environment:
      ZOOKEEPER_CLIENT_PORT: 32182
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zookeeper-target:2888:3888
    networks:
      - core
    ports:
      - 32182:32182
    hostname: zookeeper-target
    restart: on-failure

  kafka-target:
    image: confluentinc/cp-kafka:7.5.3
    container_name: kafka-target
    environment:
      KAFKA_ADVERTISED_LISTENERS: EXTERNAL_DOCKER_INTERNAL://localhost:29093,LISTENER_DOCKER_INTERNAL://kafka-target:19092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper-target:32182"
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_JMX_PORT: 9998
      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
    ports:
      - "29093:29093"
    networks:
      - core
    healthcheck:
      test: "kafka-topics --list --bootstrap-server localhost:9093 || exit 1"
      interval: 1s
      timeout: 60s
      retries: 60
    depends_on:
      - zookeeper-target
    restart: on-failure

  # Offset store
  redis:
    image: redis
    container_name: redis
    ports:
      - '6380:6379'
    healthcheck:
      test: "redis-cli ping || exit 1"
      interval: 1s
      timeout: 60s
      retries: 60
    networks:
      - core
    restart: on-failure

  # Automatically creates required kafka topics if they were not created.
  kafka-topics-creator:
    build:
      context: ../kafka-topic-creator
      dockerfile: ../kafka-topic-creator/Dockerfile
    container_name: kafka-topics-creator
    depends_on:
      zookeeper-src:
        condition: service_started
      kafka-src:
        condition: service_healthy
    environment:
      BOOTSTRAP_VALUE: "kafka-src:19092"
      KAFKA_TOPICS: "knowledge knowledge1 knowledge2"
    networks:
      - core

  # Automatically create and populates kafka topics.
  kafka-topics-populator:
    build:
      context: ../kafka-topic-populator
      dockerfile: ../kafka-topic-populator/Dockerfile
    container_name: kafka-topics-populator
    depends_on:
      zookeeper-src:
        condition: service_started
      kafka-src:
        condition: service_healthy
    environment:
      KAFKA_BROKER_SERVER : "kafka-src:19092"
      KNOWLEDGE_TOPIC: "knowledge knowledge1 knowledge2"
      KNOWLEDGE_DATA: "${KNOWLEDGE_DATA} ${KNOWLEDGE_DATA_1} ${KNOWLEDGE_DATA_2}"
      DATA_DELIMITER: "¬"
    networks:
      - core
    volumes:
      - ../input:/usr/bin/input

  # Docker image that populates target kafka topics with test data
  # Set NUMBER_OF_DATA_LOADS to 0 to switch off the test data producer
  kafka-test-data-producer:
    build:
      context: ../kafka-test-data-producer
      dockerfile: ../kafka-test-data-producer/Dockerfile
    container_name: kafka-test-data-producer
    depends_on:
      kafka-src:
        condition: service_healthy
    environment:
      KAFKA_BROKER_SERVERS : "kafka-src:19092"
      KAFKA_TOPICS: "createDataTopic1 createDataTopic2 createDataTopic3"
      COUNTRY_CODES: "GBR US FRA IRL NZ AUS US"
      COMPANY_NAMES: "ndtp.co.uk"
      SURNAMES: "Smith Johnson Williams Jones Brown Davis Miller Wilson Moore Taylor Evans McDougal"
      FIRST_NAMES: "James John Robert Michael William David Richard Charles Joseph Thomas Dylan Brian Dougal Zebedee"
      DELAY_BETWEEN_DATA_LOADS: "5"
      NUMBER_OF_DATA_LOADS: "5"
      NUMBER_OF_MESSAGES_PER_DATA_LOAD: "10"
      DATA_DELIMITER: "¬"
    networks:
      - core
    volumes:
      - ../input:/usr/bin/input

  # Docker image that loads test data from a file into a target kafka topic
  # Set NUMBER_OF_DATA_LOADS to 0 to switch off the data loader
  kafka-bulk-test-data-loader:
    build:
      context: ../kafka-bulk-test-data-loader
      dockerfile: ../kafka-bulk-test-data-loader/Dockerfile
    container_name: kafka-bulk-test-data-loader
    depends_on:
      zookeeper-src:
        condition: service_started
      kafka-src:
        condition: service_healthy
    environment:
      KAFKA_SERVERS: "kafka-src:19092"
      KAFKA_TOPICS: "bulkDataLoaderTopic1 bulkDataLoaderTopic2 bulkDataLoaderTopic3"
      DATA_FILENAME: "bulk-topic-data-1.dat"
      DATA_DELIMITER: "¬"
      DELAY_BETWEEN_DATA_LOADS: "2"
      NUMBER_OF_DATA_LOADS: "50"
    networks:
      - core
    volumes:
      - ../input:/usr/bin/input

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8082:8080"
    environment:
      # First Kafka cluster (source)
      - KAFKA_CLUSTERS_0_NAME=source
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka-src:19092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper-src:32181

      # Second Kafka cluster (target)
      - KAFKA_CLUSTERS_1_NAME=target
      - KAFKA_CLUSTERS_1_BOOTSTRAPSERVERS=kafka-target:19092
      - KAFKA_CLUSTERS_1_ZOOKEEPER=zookeeper-target:32182

      # Optional UI configs
      - SERVER_PORT=8080
      - AUTH_TYPE=DISABLED

    depends_on:
      - kafka-src
      - kafka-target
    networks:
      - core
    restart: on-failure


  minio:
    image: quay.io/minio/minio
    container_name: minio
    ports:
      - "9005:9000"   # MinIO S3 API
      - "9004:9001"   # MinIO Web Console
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    networks:
      - core
    volumes:
      - ./minio-data:/data

  azurite:
    image: mcr.microsoft.com/azure-storage/azurite
    container_name: azurite
    ports:
      - "10000:10000" # Blob service
      - "10001:10001" # Queue service
      - "10002:10002" # Table service
    command: azurite --blobHost 0.0.0.0 --queueHost 0.0.0.0 --tableHost 0.0.0.0
    networks:
      - core
    volumes:
      - ./azurite-data:/data

networks:
  core:
    name: core
