{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>This repository contributes to the development of secure, scalable, and interoperable data-sharing infrastructure. It supports NDTP\u2019s mission to enable trusted, federated, and decentralised data-sharing across organisations.</p> <p>This repository is one of several open-source components that underpin NDTP\u2019s Integration Architecture (IA)\u2014a framework designed to allow organisations to manage and exchange data securely while maintaining control over their own information. The IA is actively deployed and tested across multiple sectors, ensuring its adaptability and alignment with real-world needs.</p> <p>For a complete overview of the Integration Architecture (IA) project, please see the Integration Architecture Documentation.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Java 21</li> <li>This repo uses a maven wrapper so no installation of maven is required.</li> <li>Docker</li> <li>Git</li> <li>Management-node</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Follow these steps to get started quickly with this repository. For detailed installation, configuration, and deployment, refer to the relevant MD files.</p>"},{"location":"#1-download-and-build","title":"1. Download and Build","text":"<p>To download from the github repository run the following commands:</p> <pre><code>git clone https://github.com/National-Digital-Twin/federator.git\ncd federator  \n</code></pre> <p>To run a demo with multiple Federator clients and multiple Federator servers run the following commands from the project root directory:</p> <p>Compile the java source code:  (Replace <code>./mvnw</code> with <code>mvn</code> to use the maven without the wrapper)</p> <pre><code>./mvnw clean install\n</code></pre> <p>Build the docker containers:</p> <pre><code>docker compose --file docker/docker-compose-multiple-clients-multiple-server.yml build\n</code></pre>"},{"location":"#2-run-build-version","title":"2. Run Build Version","text":"<p>Run the docker containers:</p> <pre><code>docker compose --file docker/docker-compose-multiple-clients-multiple-server.yml up\n</code></pre> <p>You should then see the service running within docker containers. These contain multiple clients and multiple servers and their supporting services. The service will move the data from the topic(s) in the kafka-src to federated topic(s) in kafka-target.</p>"},{"location":"#3-installation","title":"3. Installation","text":"<p>Refer to INSTALLATION.md for detailed installation steps, including required dependencies and setup configurations.</p>"},{"location":"#4-uninstallation","title":"4. Uninstallation","text":"<p>For steps to remove this repository and its dependencies, see UNINSTALL.md.</p>"},{"location":"#features","title":"Features","text":"<p>The federator enables secure data exchange between Integration Architecture nodes, supporting both server (producer) and client (consumer) roles. Key features include:</p> <ul> <li>Secure, scalable data sharing using Kafka as both source and target.</li> <li>Multiple federator servers and clients per organisation for flexible deployment.</li> <li>Filtering of Kafka messages for federation is based on the <code>securityLabel</code> in the Kafka message header and the client\u2019s credentials. The default filter performs an exact match between the client\u2019s credentials and the <code>securityLabel</code> header (e.g., <code>Security-Label:nationality=GBR</code>).</li> <li>Custom filtering logic can be configured; see Configuring a Custom Filter for details.</li> <li>Communication between federator servers and clients uses gRPC over mTLS for secure, authenticated data transfer.</li> <li>Federation currently supports RDF payloads, with extensibility hooks for other data formats on a per-topic basis.</li> <li>Integration with Management-Node for centralised configuration, topic management, and authorisation.</li> <li>Redis is used for offset tracking and short-lived configuration caching.</li> </ul> <p>An overview of the Federator service architecture is shown below:</p> <p> The diagram above shows how the Federator can be used to exchange data between Integration Architecture Nodes that are running within many different organisations. Each organisation could typically run many servers (producers) and many clients (consumers) to exchange data between their Integration Architecture Nodes.</p> <p>For example within the above diagram:</p> <ul> <li>Organisation 2 (Org 2) is shown to be running two servers, with one named \"Producer Node A1\" that is sending messages to the topic named \"DP1\"</li> <li>Organisation 1 (Org 1) is shown to be running a client called \"Consumer Node B2\" which is reading the messages from the topic named \"DP1\"</li> </ul> <p>It should be further noted that this diagram shows that many servers (or producers) and many clients (or consumers) can be configured within each organisation to exchange data between their Integration Architecture Nodes.</p> <p>Additional note on connectivity and security: - Multiple Federator Producers and Consumers can exchange data across organisations using gRPC over mTLS. - As long as they are configured to talk to the same Management-Node, they will obtain compatible configuration (topics, roles, filters, endpoints) required for their data exchange. - The Management-Node, together with the Identity Provider, issues the certificates/credentials and tokens that enable mutual TLS and authorisation. - This means any number of Producers and Consumers can safely share data so long as their exchange requirements are defined in, and served by, the Management-Node.</p>"},{"location":"#exchange-data-between-ia-nodes","title":"Exchange data between IA nodes","text":"<p>The Federator is designed to allow data exchange between Integration Architecture Nodes.  Kafka brokers are used as both a source of data and a target of data that is to be moved between Integration Architecture nodes. It is run in a distributed manner with multiple servers and clients.</p> <p>A simplistic view of the federator service is described below:</p>"},{"location":"#server-producer","title":"Server (Producer)","text":"<ol> <li>A server (producer) reads messages from a knowledge topic within the source Kafka broker.</li> <li>The server is configured so that it has a list of clients and the topics that they are allowed to read the messages from.</li> <li>The server also has a configurable filter that is used to decide if a message should be sent to a client.</li> <li>The server filters the messages based on the security label in the message header.</li> <li>The server streams the selected filtered messages to the client(s) using the gRPC protocol over a network.</li> </ol>"},{"location":"#client-consumer","title":"Client (Consumer)","text":"<ol> <li>A client (consumer) connects and then authenticates with its known server(s) using the gRPC protocol.</li> <li>A client requests the list of topics that it is allowed to read from the server.</li> <li>The client then requests the messages from the server for given topic(s).</li> <li>The client reads the messages and then writes them to a target Kafka broker to a topic name that is prefixed with 'federated'</li> </ol> <p>The underlying communication protocol is gRPC which is used to communicate between the server and client at the network level.</p>"},{"location":"#architecture","title":"Architecture","text":""},{"location":"#federator-server-producer","title":"Federator Server (Producer)","text":"<p>This app starts the data federation server that starts a gRPC service.</p> <p>This process contains the federator service supplying two RPC endpoints that are called by the client:</p> <ul> <li>Get Kafka Topics (obtain topics)</li> <li>Get kafka Consumer (consume topic)</li> </ul>"},{"location":"#obtain-topics","title":"Obtain Topics","text":"<ol> <li>Is passed a user request (a client-id and key)</li> <li>Authenticate the given credentials</li> <li>Returns the topics that have been assigned to the given user.</li> </ol>"},{"location":"#consume-topic","title":"Consume Topic","text":"<ol> <li>Is passed a topic request (client-id, key, topic &amp; offset)</li> <li>Validates the given details.</li> <li>Creates a message conductor to process the topic.</li> <li>Consumes and returns messages until stopped.</li> </ol>"},{"location":"#federator-client-consumer","title":"Federator Client (Consumer)","text":"<p>A somewhat simple app it does the following:</p> <ol> <li>Obtains topic(s) from the Server</li> <li>Checks with Redis to see what the offset is for given topic</li> <li>Obtain kafka consumer from the Server</li> <li>Process messages from consumer, adding to destination topic and update Redis offset count.</li> <li>Continue (4) until stopped.    If configured, it will repeat 1-5 upon failures</li> </ol> <p>Please refer to this context diagram as an overview of the federator service and its components:</p> <p></p> <p>This diagram illustrates the main components involved in a typical deployment: - Federator Producer and Federator Consumer communicating over gRPC (mTLS). - Kafka clusters used by producers and consumers. - Redis cache used for short\u2011lived configuration and offsets/tokens. - Management-Node service that provides configuration to Federators. - Identity Provider (e.g., Keycloak) used for authentication and authorisation. - Postgres databases used by the Management-Node and Identity Provider.</p> <p>See the Architecture section below for more detail on Producer and Consumer responsibilities.</p>"},{"location":"#testing-guide","title":"Testing Guide","text":""},{"location":"#running-unit-tests","title":"Running Unit Tests","text":"<p>Navigate to the root of the project and run <code>mvn test</code> to run the tests for the repository.</p>"},{"location":"#public-funding-acknowledgment","title":"Public Funding Acknowledgment","text":"<p>This repository has been developed with public funding as part of the National Digital Twin Programme (NDTP), a UK Government initiative. NDTP, alongside its partners, has invested in this work to advance open, secure, and reusable digital twin technologies for any organisation, whether from the public or private sector, irrespective of size.</p>"},{"location":"#license","title":"License","text":"<p>This repository contains both source code and documentation, which are covered by different licenses: - Code: Originally developed by Telicent UK Ltd, now maintained by National Digital Twin Programme. Licensed under the Apache License 2.0. - Documentation: Licensed under the Open Government Licence (OGL) v3.0.</p> <p>By contributing to this repository, you agree that your contributions will be licenced under these terms.</p> <p>See LICENSE, OGL_LICENSE, and NOTICE for details.</p>"},{"location":"#security-and-responsible-disclosure","title":"Security and Responsible Disclosure","text":"<p>We take security seriously. If you believe you have found a security vulnerability in this repository, please follow our responsible disclosure process outlined in SECURITY.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions that align with the Programme\u2019s objectives. Please read our Contributing guidelines before submitting pull requests.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This repository has benefited from collaboration with various organisations. For a list of acknowledgments, see ACKNOWLEDGMENTS.</p>"},{"location":"#support-and-contact","title":"Support and Contact","text":"<p>For questions or support, check our Issues or contact the NDTP team on ndtp@businessandtrade.gov.uk.</p> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity.</p>"},{"location":"FILE_STREAMINING_README/","title":"File Streaming (Server \u2192 Client) \u2014 Federator","text":""},{"location":"FILE_STREAMINING_README/#overview","title":"Overview","text":"<p>This document explains the Federator file streaming capability, which sends files from the server to clients over gRPC as a sequence of chunks. It covers the architecture, stream/chunk protocol, configuration, offsets/resume semantics, storage providers, error handling, and testing guidance.</p> <p>Key highlights: - gRPC bidirectional-style server streaming (<code>GetFilesStream</code>) delivering <code>FileChunk</code> messages. - Deterministic chunking with a configurable <code>chunkSize</code> on the server. - End-of-file signaled via a final chunk with <code>is_last_chunk = true</code> and <code>file_checksum</code> (SHA-256). - Resume support using <code>start_sequence_id</code> to continue from a previously received point. - Producer supports multiple source providers: AWS S3 and Azure Blob Storage, plus Local file system. - Consumer supports AWS S3 as the final destination; local disk may be used for temporary assembly. - Pluggable storage providers (LOCAL, S3, Azure for producer) for reading and writing files or parts.</p>"},{"location":"FILE_STREAMINING_README/#architecture","title":"Architecture","text":"<p>At a high level, a client requests a file stream from the Federator server. The server locates and reads the source file (via <code>FileProvider</code> implementations) from S3, Azure, or Local and streams chunks to the client. The client assembles and verifies integrity using the final checksum. On the consumer side, the final destination is S3, with bucket configured in <code>client.properties</code> and the object path resolved from database configuration.</p> <pre><code>flowchart LR\n    A[Client] -- GetFilesStream request --&gt; B[gRPC FederatorService]\n    B --&gt; C[FileChunkStreamer]\n    C --&gt; D{FileProviderFactory}\n    D --&gt;|LOCAL| E[Local FS]\n    D --&gt;|S3| F[S3 Bucket]\n    D --&gt;|AZURE| Z[Azure Blob Container]\n    C -- stream FileChunk --&gt; A\n    A -.-&gt; G[Assembler]\n    G --&gt;|Finalize| H[Consumer Destination S3]\n    %% Using unlabeled dotted edges to avoid Mermaid lexical issues with label text\n    H -.-&gt; I[Bucket from client.properties]\n    H -.-&gt; J[Destination path from database]</code></pre>"},{"location":"FILE_STREAMINING_README/#server-side-components","title":"Server-side components","text":"<ul> <li><code>FederatorService.proto</code> defines <code>GetFilesStream(FileStreamRequest) returns (stream FileChunk)</code>.</li> <li><code>FileChunkStreamer</code> reads the file in <code>chunkSize</code> blocks, emits <code>FileChunk</code> messages, and sends a final last-chunk with checksum.</li> <li><code>FileProviderFactory</code> resolves the concrete <code>FileProvider</code> for the configured source type (LOCAL, S3, Azure).</li> </ul>"},{"location":"FILE_STREAMINING_README/#client-side-components","title":"Client-side components","text":"<ul> <li>A gRPC client (<code>ClientGRPCJob</code> orchestrator and related handlers) initiates the stream request, consumes chunks, writes temporary parts, and finalizes the file.</li> <li>Client storage is pluggable for temp assembly (LOCAL or S3) and final destination is S3 via implementations such as <code>S3ReceivedFileStorage</code>.</li> </ul> <pre><code>sequenceDiagram\n    participant Client\n    participant Service as FederatorService\n    participant Streamer as FileChunkStreamer\n    participant Storage as FileProvider (LOCAL/S3)\n\n    Client-&gt;&gt;Service: GetFilesStream(FileStreamRequest{start_sequence_id})\n    Service-&gt;&gt;Streamer: stream(fileSequenceId, request, observer)\n    Streamer-&gt;&gt;Storage: get(FileTransferRequest)\n    Storage--&gt;&gt;Streamer: InputStream, fileSize\n    loop for each chunk\n        Streamer--&gt;&gt;Client: FileChunk{chunk_data, chunk_index, total_chunks, file_size}\n    end\n    Streamer--&gt;&gt;Client: FileChunk{is_last_chunk=true, file_checksum}\n    Client--&gt;&gt;Client: Assemble + Verify checksum</code></pre>"},{"location":"FILE_STREAMINING_README/#tech-stack","title":"Tech Stack","text":"<ul> <li>gRPC + Protobuf (<code>FederatorService.proto</code> \u2192 generated stubs in <code>uk.gov.dbt.ndtp.grpc</code>)</li> <li>Java (server and client components)</li> <li>Storage providers: Local filesystem, AWS S3 or S3-compatible such as MinIO, Azure Blob Storage</li> <li>Redis and Kafka exist in the broader system; file streaming can be used alongside other data paths</li> </ul>"},{"location":"FILE_STREAMINING_README/#getting-started","title":"Getting Started","text":"<ol> <li>Generate and build the project:</li> <li><code>mvn clean package</code></li> <li>Start local dependencies (optional demo):</li> <li>See <code>docker/docker-grpc-resources</code> for example environments such as MinIO S3 and Azurite Azure. Start the relevant compose file if needed.</li> <li>Configure client properties (see Configuration below), especially storage provider and directories/buckets.</li> <li>Run server and client processes. The client invokes <code>GetFilesStream</code> and writes assembled files to the configured storage.</li> </ol> <pre><code>flowchart TD\n    dev[Dev Machine] --&gt;|mvn package| build[[Build]]\n    build --&gt; runServer[Run Federator Server]\n    build --&gt; runClient[Run Federator Client]\n    subgraph Optional Services\n        kafka[Kafka]\n        redis[Redis]\n        minio[MinIO or S3]\n    end\n    runServer --&gt;|serve gRPC| runClient\n    runClient --&gt;|receive chunks| minio</code></pre>"},{"location":"FILE_STREAMINING_README/#configuration","title":"Configuration","text":"<p>Primary client settings are in <code>src/configs/client.properties</code>: - Storage provider   - <code>client.files.storage.provider</code> = <code>LOCAL</code> | <code>S3</code> (default <code>LOCAL</code>) - Local storage   - <code>client.files.temp.dir</code> \u2014 directory for received files and temporary parts</p>"},{"location":"FILE_STREAMINING_README/#local-temp-directory-clientfilestempdir","title":"Local temp directory (<code>client.files.temp.dir</code>)","text":"<ul> <li>Purpose: This directory is used by the client to assemble incoming chunks. During transfer, parts are written to <code>&lt;base&gt;/.parts/&lt;file&gt;.&lt;seq&gt;.part</code>. On the final chunk, the <code>.part</code> file is moved to <code>&lt;base&gt;/&lt;file&gt;</code> and then handed off to the configured storage provider (LOCAL or S3).</li> <li>Defaults: If the property is blank or missing, it falls back to <code>${java.io.tmpdir}/federator-files</code>.</li> <li>Cleanup behavior:</li> <li>Success to S3: The assembled local file is best-effort deleted by the S3 storage provider after upload.</li> <li>Upload failure: The S3 provider also attempts to delete the assembled local file and does not advance offsets.</li> <li>Integrity failure (checksum/size): The assembler deletes the <code>.part</code> file and aborts.</li> <li>Interruption/crash: A stale <code>.part</code> may remain; you can safely remove <code>*.part</code> files that are older than your retention window.</li> <li>Production guidance: For large files, mount this path on durable storage with sufficient space. Recommended capacity = peak concurrent files \u00d7 maximum file size + 20-30% headroom. Ensure the process user has read/write permissions and monitor disk usage.</li> <li>S3 settings (also used by server-side components in some deployments)</li> <li><code>files.s3.bucket</code></li> <li><code>aws.s3.region</code></li> <li><code>aws.s3.access.key.id</code></li> <li><code>aws.s3.secret.access.key</code></li> <li><code>aws.s3.endpoint.url</code> (S3-compatible systems like MinIO)</li> <li><code>aws.s3.profile</code> (SSO/local profiles)</li> </ul>"},{"location":"FILE_STREAMINING_README/#s3-settings-when-to-set-and-when-to-leave-blank","title":"S3 settings \u2014 when to set and when to leave blank","text":"<p>The following guidance explains which S3 properties must be set or left blank for common deployment scenarios. The client uses an AWS SDK\u2013style credential resolution (via <code>S3ClientFactory</code>) and supports static keys, shared profiles/SSO, or instance/role credentials. Only configure one credentials method at a time.</p> <ul> <li>files.s3.bucket</li> <li>Set: Always required for the consumer because the final destination is S3 (including S3-compatible like MinIO). The bucket may be a real AWS bucket or a MinIO bucket name.</li> <li> <p>Blank: Only permissible if the consumer is not writing to S3 at all (e.g., local testing where the consumer is not invoked or S3 writes are disabled). In normal consumer runs, do not leave blank.</p> </li> <li> <p>aws.s3.region</p> </li> <li>Set: Recommended for AWS. If using static keys or profile, set the AWS region (e.g., <code>us-east-1</code>, <code>eu-west-2</code>). For MinIO or other S3-compatible endpoints, set to a placeholder like <code>us-east-1</code> if the SDK requires a non-empty region.</li> <li> <p>Blank: Acceptable when the region is resolvable from the environment, profile, or instance metadata. Not recommended for local dev as it can be ambiguous.</p> </li> <li> <p>aws.s3.access.key.id and aws.s3.secret.access.key</p> </li> <li>Set: For static IAM user credentials or S3-compatible systems like MinIO. Use together as a pair.</li> <li> <p>Blank: When using <code>aws.s3.profile</code> (shared config/SSO) or IAM role credentials on EC2/ECS/Lambda (instance/role provider). Also leave blank if relying on environment variables or default provider chain that supplies credentials.</p> </li> <li> <p>aws.s3.endpoint.url</p> </li> <li>Set: Only for S3-compatible systems (e.g., MinIO). Example: <code>http://localhost:9000</code>.</li> <li> <p>Blank: For real AWS S3 \u2014 do not set an endpoint.</p> </li> <li> <p>aws.s3.profile</p> </li> <li>Set: When using shared AWS config/credentials profile (including AWS SSO). Example: <code>default</code> or <code>my-sso-profile</code>.</li> <li>Blank: For static keys, IAM role credentials, or MinIO when providing access/secret directly. Do not set a profile and static keys at the same time.</li> </ul>"},{"location":"FILE_STREAMINING_README/#common-scenarios-and-property-examples","title":"Common scenarios and property examples","text":"<p>1) AWS S3 with static access keys <pre><code>files.s3.bucket=my-prod-bucket\naws.s3.region=eu-west-2\naws.s3.access.key.id=AKIA...\naws.s3.secret.access.key=...\naws.s3.endpoint.url=\naws.s3.profile=\n</code></pre></p> <p>2) AWS S3 with shared profile (including SSO) <pre><code>files.s3.bucket=my-dev-bucket\naws.s3.region=eu-west-2            # optional if region is in the profile; recommended to set\naws.s3.access.key.id=\naws.s3.secret.access.key=\naws.s3.endpoint.url=\naws.s3.profile=my-sso-profile\n</code></pre></p> <p>3) AWS S3 with IAM role on EC2/ECS/Lambda <pre><code>files.s3.bucket=service-bucket\naws.s3.region=eu-west-2            # or leave blank to auto-resolve from instance metadata\naws.s3.access.key.id=\naws.s3.secret.access.key=\naws.s3.endpoint.url=\naws.s3.profile=\n</code></pre></p> <p>4) S3\u2011compatible (MinIO) local development <pre><code>files.s3.bucket=minio-bucket\naws.s3.region=us-east-1            # commonly required placeholder for MinIO\naws.s3.access.key.id=minioadmin\naws.s3.secret.access.key=minioadmin\naws.s3.endpoint.url=http://localhost:9000\naws.s3.profile=\n</code></pre></p>"},{"location":"FILE_STREAMINING_README/#dos-and-donts","title":"Dos and Don'ts","text":"<ul> <li>Do configure only one credential source: static keys OR profile/SSO OR IAM role. Mixing profile and keys can cause ambiguous resolution.</li> <li>Do set <code>files.s3.bucket</code> for all consumer runs; the consumer writes the final assembled file to this bucket with the object key/prefix coming from the database configuration.</li> <li>Don\u2019t set <code>aws.s3.endpoint.url</code> for real AWS S3; it is meant for S3\u2011compatible endpoints like MinIO.</li> <li> <p>Don\u2019t leave both keys and profile populated; choose exactly one.</p> </li> <li> <p>Streaming (server-side)</p> </li> <li><code>file.stream.chunk.size</code> \u2014 chunk size in bytes used by the server when streaming files. If not set, defaults to <code>1000</code> bytes. This is read by the server (e.g., in <code>FileKafkaEventMessageProcessor</code>) to construct <code>FileChunkStreamer</code>.</li> </ul> <p>Producer vs Consumer specifics: - Producer   - Supports reading from S3, Azure, or Local depending on the message <code>sourceType</code>.   - For Azure, use Azurite or a real Azure Storage account; provider configuration is resolved by the platform components behind <code>FileProviderFactory</code>. - Consumer   - Final destination is AWS S3 only. Temporary parts may be written to local disk depending on <code>client.files.storage.provider</code>.   - The S3 bucket name comes from <code>files.s3.bucket</code> in <code>client.properties</code>.   - The S3 object destination key or prefix comes from database-resident consumer configuration.</p> <p>Networking and TLS (if enabled): - <code>client.p12FilePath</code>, <code>client.p12Password</code>, <code>client.truststoreFilePath</code>, <code>client.truststorePassword</code></p> <p>Other relevant system options include Redis/Kafka groups, retry back-off, and topic prefixing used elsewhere in the broader system.</p>"},{"location":"FILE_STREAMINING_README/#topic-message-schema","title":"Topic Message Schema","text":"<p>Messages pushed to the topic use a JSON payload indicating the source of the file to stream. Example:</p> <pre><code>{\n  \"sourceType\": \"S3\",\n  \"storageContainer\": \"s3-heg\",\n  \"path\": \"intellij-java-google-style.xml\"\n}\n</code></pre> <p>Fields: - <code>sourceType</code> \u2014 the origin provider of the file. Allowed values: <code>S3</code>, <code>AZURE</code>, <code>LOCAL</code> (case-sensitive; all caps). - <code>storageContainer</code> \u2014 container name for the file source:   - For <code>S3</code>: the bucket name.   - For <code>AZURE</code>: the blob container name.   - For <code>LOCAL</code>: optional label; actual base directory is resolved by the Local provider configuration. - <code>path</code> \u2014 the object key or file path within the container:   - For <code>S3</code>: the object key within the bucket.   - For <code>AZURE</code>: the blob name within the container.   - For <code>LOCAL</code>: the file path relative to the configured base directory.</p> <p>Notes: - The message schema selects the producer-side <code>FileProvider</code>. The gRPC API remains the same regardless of source. - Consumer S3 destination is configured separately: bucket from properties, destination key or prefix from database configuration.</p>"},{"location":"FILE_STREAMINING_README/#grpc-apis","title":"gRPC APIs","text":"<p>Defined in <code>src/main/proto/FederatorService.proto</code>:</p> <ul> <li><code>rpc GetFilesStream(FileStreamRequest) returns (stream FileChunk)</code></li> <li>Request<ul> <li><code>Topic</code> \u2014 optional logical topic or collection identifier used by the server to choose files</li> <li><code>start_sequence_id</code> \u2014 resume point; <code>0</code> means from the beginning</li> </ul> </li> <li>Response stream: <code>FileChunk</code><ul> <li><code>file_name</code> \u2014 logical or source name</li> <li><code>chunk_data</code> \u2014 bytes for the data chunk (omitted for the final last-chunk)</li> <li><code>chunk_index</code> \u2014 zero-based index for the chunk</li> <li><code>total_chunks</code> \u2014 total count (known for data chunks; repeated on final chunk)</li> <li><code>is_last_chunk</code> \u2014 <code>true</code> when the final, metadata-only chunk is sent</li> <li><code>file_checksum</code> \u2014 SHA-256 of the full file (only set on the last chunk)</li> <li><code>file_size</code> \u2014 size in bytes of the full file</li> <li><code>file_sequence_id</code> \u2014 monotonically increasing identifier for files in a stream</li> </ul> </li> </ul> <pre><code>classDiagram\n    class FileStreamRequest {\n      string Topic\n      int64 start_sequence_id\n    }\n    class FileChunk {\n      string file_name\n      bytes chunk_data\n      int32 chunk_index\n      int32 total_chunks\n      bool is_last_chunk\n      string file_checksum\n      int64 file_size\n      int64 file_sequence_id\n    }</code></pre>"},{"location":"FILE_STREAMINING_README/#storage-providers","title":"Storage Providers","text":"<p>Server read path uses <code>FileProviderFactory</code> \u2192 <code>FileProvider</code>: - LOCAL: reads from a local filesystem path. - S3: reads from a configured S3 bucket and key. - Azure: reads from a configured Azure Blob container and blob name.</p> <p>Client write path uses pluggable storage implementations: - LOCAL: write parts to <code>client.files.temp.dir</code>, then assemble. - S3: <code>S3ReceivedFileStorage</code> writes to S3, buffering parts as needed and finalizing to an object. Final destination is S3 only; Azure is not supported for the consumer destination.</p> <pre><code>flowchart LR\n    FP[FileProviderFactory] --&gt;|LOCAL| L[LocalFileProvider]\n    FP --&gt;|S3| S[S3FileProvider]\n    FP --&gt;|AZURE| A[AzureBlobFileProvider]\n    subgraph Client Storage\n      CS1[LocalReceivedFileStorage]\n      CS2[S3ReceivedFileStorage]\n    end</code></pre>"},{"location":"FILE_STREAMINING_README/#streaming-chunking","title":"Streaming &amp; Chunking","text":"<ul> <li>Chunk size is configured server-side when constructing <code>FileChunkStreamer</code> (e.g., via application configuration). It determines throughput and memory usage trade-offs.</li> <li>For each chunk:</li> <li><code>chunk_index</code> increments from <code>0</code>.</li> <li><code>total_chunks</code> is included with every message to aid progress calculation.</li> <li><code>file_size</code> is included.</li> <li>The server maintains a SHA-256 digest over all chunk bytes.</li> <li>After the last data chunk, the server sends a final control message with <code>is_last_chunk = true</code>, <code>file_checksum</code> set, and the next <code>chunk_index</code> value (i.e., <code>lastChunkIndex</code>).</li> </ul>"},{"location":"FILE_STREAMINING_README/#client-assembly-lifecycle","title":"Client assembly lifecycle","text":"<ul> <li>Incoming chunks are written to <code>&lt;client.files.temp.dir&gt;/.parts</code> as a single growing <code>.part</code> file per stream/file.</li> <li>On the last chunk (after checksum/size verification), the <code>.part</code> is moved atomically to <code>&lt;client.files.temp.dir&gt;/&lt;fileName&gt;</code>.</li> <li>The configured client storage provider is invoked:</li> <li>LOCAL: the final file remains under <code>client.files.temp.dir</code>.</li> <li>S3: the final file is uploaded and then deleted locally by the S3 provider (best-effort).</li> </ul> <p>Provider note: Chunking and checksumming are provider-agnostic; the same streaming protocol applies whether the source is <code>S3</code>, <code>Azure</code>, or <code>Local</code>.</p> <pre><code>sequenceDiagram\n    participant S as Server\n    participant C as Client\n    loop chunks\n        S--&gt;&gt;C: FileChunk chunk_data, chunk_index\n        C--&gt;&gt;C: write part and update progress\n    end\n    S--&gt;&gt;C: FileChunk is_last_chunk=true, file_checksum\n    C--&gt;&gt;C: verify checksum equals computed SHA-256</code></pre>"},{"location":"FILE_STREAMINING_README/#offset-management","title":"Offset Management","text":"<p>Two levels of progress are commonly tracked: 1) File sequence \u2014 <code>file_sequence_id</code> identifies each file in order. 2) In-file progress \u2014 <code>chunk_index</code> for partial assembly.</p> <p>Clients can persist <code>file_sequence_id</code> and optionally last fully assembled file. To resume, set <code>start_sequence_id</code> in the request.</p> <pre><code>flowchart TD\n    A[Start] --&gt; B{Have saved file_sequence_id?}\n    B -- No --&gt; C[Use 0 from beginning]\n    B -- Yes --&gt; D[Set start_sequence_id = saved]\n    C --&gt; E[Request stream]\n    D --&gt; E[Request stream]\n    E --&gt; F[Receive chunks]\n    F --&gt; G{Final chunk received?}\n    G -- No --&gt; F\n    G -- Yes --&gt; H[Verify checksum]\n    H --&gt; I{OK?}\n    I -- Yes --&gt; J[Persist new sequence id]\n    I -- No --&gt; K[Initiate retry/re-fetch]</code></pre> <p>If the client also tracks partial assembly by <code>chunk_index</code>, it can discard duplicates upon resume and continue appending, or restart the file depending on policy.</p>"},{"location":"FILE_STREAMINING_README/#error-handling-retry","title":"Error Handling &amp; Retry","text":"<ul> <li>Server side: Exceptions during streaming produce gRPC <code>Status.INTERNAL</code> with cause. The server logs the error and terminates the stream.</li> <li>Client side should implement retry with exponential back-off. Suggested options mirror standard properties used elsewhere in the project (e.g., <code>retries.max_attempts</code>, <code>retries.initial_backoff</code>, <code>retries.max_backoff</code>, <code>retries.forever</code>).</li> <li>Integrity verification: If checksum mismatches, the client must treat the file as incomplete/corrupt and retry from a safe point.</li> </ul> <p>Additional considerations: - Azure producer source: handle authentication and container resolution errors, including SAS token expiry, missing containers, or network timeouts. - Consumer destination: if the bucket from <code>client.properties</code> is missing or the destination path from the database cannot be resolved, treat as configuration error and fail fast with clear logs for remediation.</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Streaming\n    Streaming --&gt; Streaming: transient error + retry\n    Streaming --&gt; Failed: unrecoverable error or max attempts reached\n    Streaming --&gt; Verifying: last-chunk received\n    Verifying --&gt; Completed: checksum OK\n    Verifying --&gt; Failed: checksum mismatch</code></pre>"},{"location":"FILE_STREAMINING_README/#testing","title":"Testing","text":"<ul> <li>Unit tests</li> <li>Validate chunk counting and boundary conditions (exact multiples of <code>chunkSize</code>, very small files, empty files).</li> <li>Verify checksum emission matches a standalone SHA-256 of the file.</li> <li>Ensure <code>is_last_chunk</code> semantics and <code>total_chunks</code> consistency.</li> <li>Integration tests</li> <li>End-to-end stream with LOCAL storage.</li> <li>End-to-end stream with MinIO S3 using <code>aws.s3.endpoint.url</code> and <code>files.s3.bucket</code>.</li> <li>End-to-end stream with Azurite Azure as producer source; consumer still writes to S3.</li> <li>Resume behavior using <code>start_sequence_id</code>.</li> <li>Validate topic JSON schema parsing and routing to correct <code>FileProvider</code> based on <code>sourceType</code>.</li> <li>Performance testing</li> <li>Use <code>docker/docker-grpc-resources/performance-tests</code> scaffolding to drive large files and different chunk sizes.</li> </ul>"},{"location":"FILE_STREAMINING_README/#project-structure","title":"Project Structure","text":"<p>Key locations related to file streaming: - Protobuf: <code>src/main/proto/FederatorService.proto</code> - Server streaming: <code>src/main/java/.../server/processor/file/FileChunkStreamer.java</code> - Client gRPC job/handlers: <code>src/main/java/.../client/jobs/handlers/ClientGRPCJob.java</code> - Client storage (S3 example): <code>src/main/java/.../client/storage/impl/S3ReceivedFileStorage.java</code> - S3 client factory: <code>src/main/java/.../common/storage/provider/file/client/S3ClientFactory.java</code> - Configuration: <code>src/configs/client.properties</code> - Docker examples: <code>docker/docker-grpc-resources/*</code> including <code>azurite-data</code> for Azure local dev and <code>minio-data</code> for S3-compatible local dev</p> <pre><code>flowchart LR\n    Proto[FederatorService.proto] --&gt; Gen[Generated gRPC Stubs]\n    Gen --&gt; Server[FileChunkStreamer]\n    Gen --&gt; Client[ClientGRPCJob]\n    Client --&gt; CStore[Client Storage LOCAL or S3]\n    Server --&gt; SProv[FileProviderFactory]\n    SProv --&gt; LProv[Local Provider]\n    SProv --&gt; S3Prov[S3 Provider]\n    SProv --&gt; AzProv[Azure Provider]</code></pre>"},{"location":"authentication/","title":"Authentication Configuration","text":"<p>Repository: <code>federator</code> Description: <code>authentication configuration</code></p>"},{"location":"authentication/#federator-authentication-and-authorization-process","title":"Federator Authentication and Authorization Process","text":"<p>Federator uses mutual TLS (mTLS) and JWT-based authentication and authorization throughout all client-server and client-IDP interactions.</p>"},{"location":"authentication/#mtls-authentication","title":"mTLS Authentication","text":"<ul> <li>Client to IDP:</li> <li>When a client starts, it authenticates to the Identity Provider (IDP, e.g., Keycloak) using mTLS.</li> <li>The client presents its certificate to the IDP, which verifies the identity and issues a JWT token.</li> <li> <p>All properties for mTLS (keystore, truststore, passwords) are configured in <code>common-configuration.properties</code>.</p> </li> <li> <p>Client to Server (Producer):</p> </li> <li>The client establishes a secure connection to each producer/server using mTLS.</li> <li>Both client and server verify each other's certificates, ensuring mutual trust and secure communication.</li> </ul>"},{"location":"authentication/#jwt-token-acquisition-and-usage","title":"JWT Token Acquisition and Usage","text":"<ul> <li>After successful mTLS authentication with the IDP, the client receives a JWT token.</li> <li>The JWT token is attached to every request from the client to the management node and producer servers using an Auth Client Interceptor.</li> <li>The token contains claims that identify the client and its permissions.</li> </ul>"},{"location":"authentication/#server-side-authorization","title":"Server-Side Authorization","text":"<ul> <li>When a server receives a request, it validates the JWT token:</li> <li>Verifies the token signature using the IDP's JWKS endpoint.</li> <li>Checks the token's expiration and other standard claims.</li> <li>Authorization: The server checks the <code>aud</code> (audience) claim in the JWT to ensure the token is intended for this server or service. Only requests with valid audience claims are authorized to access protected resources.</li> <li>This process ensures that only authenticated and authorized clients can access server data and operations.</li> </ul>"},{"location":"authentication/#authentication-authorization-sequence-diagram","title":"Authentication &amp; Authorization Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant Client\n    participant IDP\n    participant ManagementNode\n    participant ProducerServer\n\n    Client-&gt;&gt;IDP: Authenticate using mTLS, request JWT\n    IDP--&gt;&gt;Client: Return JWT token\n    Client-&gt;&gt;ManagementNode: Send request with JWT (Auth Client Interceptor)\n    ManagementNode-&gt;&gt;IDP: Validate JWT signature and claims\n    IDP--&gt;&gt;ManagementNode: Confirm JWT validity\n    ManagementNode-&gt;&gt;ManagementNode: Check 'aud' claim for authorization\n    alt Authorized\n        ManagementNode--&gt;&gt;Client: Grant access / return config\n    else Not authorized\n        ManagementNode--&gt;&gt;Client: Deny access\n    end\n    Client-&gt;&gt;ProducerServer: Send request with JWT (Auth Client Interceptor)\n    ProducerServer-&gt;&gt;IDP: Validate JWT signature and claims\n    IDP--&gt;&gt;ProducerServer: Confirm JWT validity\n    ProducerServer-&gt;&gt;ProducerServer: Check 'aud' claim for authorization\n    alt Authorized\n        ProducerServer--&gt;&gt;Client: Grant access / return data\n    else Not authorized\n        ProducerServer--&gt;&gt;Client: Deny access\n    end</code></pre>"},{"location":"authentication/#configuration-summary","title":"Configuration Summary","text":"<ul> <li>All mTLS and JWT properties are set in <code>common-configuration.properties</code>:</li> <li><code>idp.mtls.enabled</code>, <code>idp.keystore.path</code>, <code>idp.keystore.password</code>, <code>idp.truststore.path</code>, <code>idp.truststore.password</code>, <code>idp.jwks.url</code>, <code>idp.token.url</code>, <code>idp.client.id</code>, etc.</li> <li>All authentication and authorization is handled via mTLS and JWT tokens.</li> </ul>"},{"location":"authentication/#example-flow","title":"Example Flow","text":"<ol> <li>Client Startup:</li> <li>Loads configuration files and mTLS credentials.</li> <li>Authenticates to IDP using mTLS and requests JWT.</li> <li>JWT Token Usage:</li> <li>Attaches JWT to all requests to management node and producer servers.</li> <li>Server Validation:</li> <li>Validates JWT signature and claims, including <code>aud</code> for authorization.</li> <li>Allows or denies access based on token validity and audience.</li> </ol> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"client-configuration/","title":"Client Configuration","text":"<p>Repository: <code>federator</code> Description: <code>how to configure and run a federator client</code></p>"},{"location":"client-configuration/#overview","title":"Overview","text":"<p>This document describes the configuration properties for the Federator client, including connection settings, job parameters, caching, management node, dynamic configuration, and common configuration options shared across Federator components.</p>"},{"location":"client-configuration/#configuration-files","title":"Configuration Files","text":"<ul> <li>client.properties: Main client configuration. Location defined by the <code>FEDERATOR_CLIENT_PROPERTIES</code> environment variable.</li> <li>common-configuration.properties: Contains properties shared by both server and client. Location defined by the <code>common.configuration</code> property in <code>client.properties</code>.</li> </ul>"},{"location":"client-configuration/#client-properties","title":"Client Properties","text":"Property Description <code>kafka.sender.defaultKeySerializerClass</code> the default key serializer class <code>kafka.sender.defaultValueSerializerClass</code> the default value serializer class <code>kafka.bootstrapServers</code> the bootstrap server for the target Kafka instance <code>kafka.topic.prefix</code> the prefix for the target Kafka topics <code>kafka.consumerGroup</code> the consumer group for the client <code>kafka.pollDuration</code> duration to poll for messages (ms) - controls how long the client waits for messages before processing <code>kafka.pollRecords</code> number of records to poll per request - controls batching of messages for processing <code>kafka.additional.*</code> a repeatable optional property to set any additional kafka producer properties. These properties names must match with what kafka expects after the prefix has been removed <code>redis.host</code> the host for the redis cache <code>redis.port</code> the port for the redis cache <code>redis.tls.enabled</code> a flag to indicate if TLS is enabled for the redis cache <code>redis.username</code> the username for authenticating connections when redis Access Control List is being used. This can be left empty if either authentication is not required, or if redis only has <code>requirepass</code> enabled <code>redis.password</code> the password to be used for authenticating connections to redis. If either authentication is not required this can be left blank <code>redis.aes.key</code> if set, this will be used to encrypt values stored in Redis. The value must be Base64 and decode to 16, 24, or 32 bytes. <code>consumer.inactivity.timeout</code> duration of inactivity (ISO-8601, e.g. PT30S) before the client disconnects from the server due to inactivity <code>management.node.host</code> the hostname of the management node for client coordination and monitoring <code>management.node.port</code> the port of the management node for client coordination and monitoring"},{"location":"client-configuration/#running-the-client","title":"Running the Client","text":"<p>To run the Federator client, follow these steps:</p> <ol> <li>Prepare Configuration Files:</li> <li>Ensure <code>client.properties</code> contains all required client-specific settings.</li> <li>Ensure <code>common-configuration.properties</code> contains shared authentication and security properties.</li> <li> <p>Set the <code>common.configuration</code> property in <code>client.properties</code> to the path of your <code>common-configuration.properties</code> file.</p> </li> <li> <p>Set Environment Variables:</p> </li> <li><code>FEDERATOR_CLIENT_PROPERTIES</code>: Path to your <code>client.properties</code> file.</li> </ol> <p>Example (Linux/zsh):    <pre><code>export FEDERATOR_CLIENT_PROPERTIES=/path/to/client.properties\n</code></pre></p> <ol> <li>Run the Client:</li> <li>If using Java directly:      <pre><code>java -jar federator-client.jar\n</code></pre></li> <li> <p>If using Docker:      <pre><code>docker run --env FEDERATOR_CLIENT_PROPERTIES=/config/client.properties \\\n           -v /local/config:/config \\\n           federator-client:latest\n</code></pre></p> </li> <li> <p>Verify Startup:</p> </li> <li>Check logs for successful startup and connection to the management node.</li> <li>Ensure jobs are scheduled and connections to producers/servers are established as expected.</li> </ol> <p>Note: - All configuration properties must be set in the respective files. Properties cannot be overridden by environment variables. - The client will dynamically obtain server connection details from the management node as described above. - The location of <code>common-configuration.properties</code> is set by the <code>common.configuration</code> property in <code>client.properties</code>.</p>"},{"location":"client-configuration/#common-configuration-properties","title":"Common Configuration Properties","text":"<p>Common properties are defined in <code>common-configuration.properties</code> and may include:</p> Property Description <code>idp.mtls.enabled</code> Enable mutual TLS for Identity Provider (IDP) communication (<code>true</code>/<code>false</code>) <code>idp.client.secret</code> OAuth2 client secret (used when mTLS is disabled) <code>idp.jwks.url</code> JWKS URL of the IDP (used for verifying JWT signatures) <code>idp.token.url</code> Token endpoint URL of the IDP (used to fetch OAuth2 tokens) <code>idp.token.backoff</code> Backoff time in milliseconds before retrying a failed token request (default: 1000 ms) <code>idp.client.id</code> OAuth2 client ID (registered with the IDP) <code>idp.keystore.path</code> Path to client keystore file (PKCS12 or JKS) for mutual TLS <code>idp.keystore.password</code> Password for the client keystore <code>idp.truststore.path</code> Path to truststore containing the IDP's root CA or certificate chain <code>idp.truststore.password</code> Password for the truststore ... Other shared properties as required <p>These settings ensure secure authentication and authorisation between Federator components and the Identity Provider, supporting both mTLS and OAuth2 flows.</p>"},{"location":"client-configuration/#dynamic-consumer-configuration-via-management-node","title":"Dynamic Consumer Configuration via Management Node","text":"<p>Federator clients dynamically obtain their server connection details from the management node. The workflow is as follows:</p> <ol> <li>JWT Token Acquisition via mTLS:</li> <li>The client authenticates with the Identity Provider (IDP, e.g., Keycloak) using mutual TLS (mTLS) and obtains a JWT token.</li> <li> <p>Relevant properties for mTLS and OAuth2 are set in <code>common-configuration.properties</code> (see above).</p> </li> <li> <p>Management Node Authentication:</p> </li> <li>The client uses the JWT token to authenticate with the management node (configured via <code>management.node.host</code> and <code>management.node.port</code>).</li> <li> <p>The management node returns a JSON configuration describing available producers/servers and their products/topics.</p> </li> <li> <p>Dynamic Server Connection:</p> </li> <li>The client parses the returned JSON and connects to each producer/server listed, using the provided host, port, TLS, and topic details to fetch data.</li> </ol>"},{"location":"client-configuration/#example-management-node-response","title":"Example Management Node Response","text":"<pre><code>{\n    \"clientId\": \"FEDERATOR_HEG\",\n    \"producers\": [\n        {\n            \"products\": [\n                {\n                    \"name\": \"BrownfieldLandAvailability\",\n                    \"topic\": \"topic.BrownfieldLandAvailability\",\n                    \"consumers\": null\n                }\n            ],\n            \"name\": \"HEG-PRODUCER-1\",\n            \"description\": \"HEG Producer 1\",\n            \"active\": true,\n            \"host\": \"localhost\",\n            \"port\": 9001,\n            \"tls\": true,\n            \"idpClientId\": \"FEDERATOR_HEG\"\n        },\n        {\n            \"products\": [\n                {\n                    \"name\": \"PendingPlanningApplications\",\n                    \"topic\": \"topic.PendingPlanningApplications\",\n                    \"consumers\": null\n                }\n            ],\n            \"name\": \"BCC-PRODUCER-1\",\n            \"description\": \"BCC Producer 1\",\n            \"active\": true,\n            \"host\": \"localhost\",\n            \"port\": 9001,\n            \"tls\": true,\n            \"idpClientId\": \"FEDERATOR_BCC\"\n        }\n    ]\n}\n</code></pre> <p>Each producer in the response represents a server the client can connect to. The client uses the <code>host</code>, <code>port</code>, <code>tls</code>, and <code>idpClientId</code> fields to establish secure connections and fetch data from the specified topics.</p> <p>Required Properties: - <code>management.node.host</code>: Hostname of the management node. - <code>management.node.port</code>: Port of the management node. - mTLS and OAuth2 properties in <code>common-configuration.properties</code> for JWT acquisition.</p> <p>This approach allows clients to be dynamically configured and managed, supporting secure, scalable, and flexible data integration.</p>"},{"location":"client-configuration/#job-parameters-job-configuration","title":"Job Parameters &amp; Job Configuration","text":"<p>Job parameters control how the client processes messages and interacts with the server. Key properties include:</p> <ul> <li><code>kafka.pollDuration</code>: How long the client waits for messages before processing (in milliseconds).</li> <li><code>kafka.pollRecords</code>: Number of records to poll per request, controlling batching.</li> <li><code>consumer.inactivity.timeout</code>: Duration of inactivity before disconnecting from the server (ISO-8601 format, e.g. PT30S).</li> </ul> <p>These parameters can be tuned for performance and resource management.</p>"},{"location":"client-configuration/#job-scheduler","title":"Job Scheduler","text":"<p>Federator clients use a Job Scheduler to manage the lifecycle of jobs that control client connections and data fetching. The Job Scheduler supports:</p> <ul> <li>Add Jobs: Schedules new jobs (immediate or recurring) based on dynamic configuration from the management node. Each job may represent a connection to a producer/server and a data fetch operation for a specific topic.</li> <li>Delete Jobs: Removes scheduled jobs when they are no longer needed, causing the client to disconnect from the associated server or stop fetching data for a topic.</li> <li>Refresh Jobs: Synchronizes the set of active jobs with the latest configuration from the management node. This ensures the client only maintains connections and fetches data for currently relevant producers/servers and topics.</li> </ul> <p>The Job Scheduler enables real-time adaptation to changes in available producers/servers and their products/topics, supporting dynamic, scalable, and flexible client operation.</p>"},{"location":"client-configuration/#caching","title":"Caching","text":"<p>Federator clients can use Redis for caching. Relevant properties:</p> <ul> <li><code>redis.host</code>: Redis cache host.</li> <li><code>redis.port</code>: Redis cache port.</li> <li><code>redis.tls.enabled</code>: Enable TLS for Redis.</li> <li><code>redis.username</code>: Username for Redis ACL authentication.</li> <li><code>redis.password</code>: Password for Redis authentication.</li> <li><code>redis.aes.key</code>: Base64-encoded AES key for encrypting cached values (must decode to 16, 24, or 32 bytes).</li> </ul>"},{"location":"client-configuration/#management-node","title":"Management Node","text":"<p>Clients may connect to a management node for coordination, monitoring, and control. Relevant properties:</p> <ul> <li><code>management.node.host</code>: Hostname of the management node.</li> <li><code>management.node.port</code>: Port of the management node.</li> </ul>"},{"location":"client-configuration/#auth-client-interceptor","title":"Auth Client Interceptor","text":"<p>Federator clients use an Auth Client Interceptor to ensure secure, authenticated communication with servers (producers). The interceptor automatically attaches the JWT token\u2014obtained from the Identity Provider (IDP) using mTLS\u2014to every outgoing client request to servers. This mechanism:</p> <ul> <li>Ensures each request is authenticated and authorized according to the security policies of the management node and producers.</li> <li>Integrates seamlessly with dynamic configuration and the job scheduler, so all connections established as a result of job scheduling or configuration updates are properly secured.</li> <li>Relies on the mTLS and OAuth2 properties in <code>common-configuration.properties</code> for token acquisition and management.</li> </ul> <p>No additional configuration is required for the interceptor beyond the standard authentication properties. This approach guarantees that all client-server interactions are protected and compliant with Federator's security requirements.</p>"},{"location":"client-configuration/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Client\n    participant ConfigFiles\n    participant IDP\n    participant ManagementNode\n    participant JobScheduler\n    participant ProducerServer\n\n    User-&gt;&gt;Client: Start client process\n    Client-&gt;&gt;ConfigFiles: Load client.properties\n    Client-&gt;&gt;ConfigFiles: Load common-configuration.properties (via common.configuration)\n    Client-&gt;&gt;IDP: Authenticate using mTLS, request JWT\n    IDP--&gt;&gt;Client: Return JWT token\n    Client-&gt;&gt;ManagementNode: Authenticate with JWT, request dynamic config\n    ManagementNode--&gt;&gt;Client: Return producer/server config JSON\n    Client-&gt;&gt;JobScheduler: Schedule jobs for each producer/topic\n    loop For each job\n        JobScheduler-&gt;&gt;ProducerServer: Establish connection, fetch data\n        ProducerServer--&gt;&gt;JobScheduler: Return data\n    end\n    JobScheduler-&gt;&gt;Client: Update job status, manage connections</code></pre>"},{"location":"client-configuration/#c4-diagram","title":"C4 Diagram","text":"<p>A C4 diagram provides a high-level architectural view of the Federator client and its interactions.</p> <pre><code>%%{init: { 'theme': 'default' }}%%\nC4Container\n\nPerson(user, \"User\", \"Initiates and monitors client operation\")\nSystem(client, \"Federator Client\", \"Main client application\")\nContainer(config, \"Configuration Files\", \"Properties Files\", \"client.properties, common-configuration.properties\")\nContainer(idp, \"Identity Provider (IDP)\", \"OAuth2/Keycloak\", \"Issues JWT tokens via mTLS\")\nContainer(mgmt, \"Management Node\", \"Federator Management Node\", \"Provides dynamic configuration and job info\")\nContainer(job, \"Job Scheduler\", \"JobRunr/Quartz\", \"Schedules, refreshes, and deletes jobs\")\nContainer(producer, \"Producer Server\", \"Kafka/gRPC Server\", \"Provides data streams/topics\")\nContainer(redis, \"Redis Cache\", \"Redis\", \"Caching for client operations\")\n\nRel(user, client, \"Starts, configures, and monitors\")\nRel(client, config, \"Loads configuration\")\nRel(client, idp, \"Authenticates via mTLS, requests JWT\")\nRel(client, mgmt, \"Authenticates with JWT, requests dynamic config\")\nRel(mgmt, client, \"Returns producer/server config JSON\")\nRel(client, job, \"Schedules jobs for each producer/topic\")\nRel(job, producer, \"Connects, fetches data\")\nRel(client, redis, \"Uses for caching\")\n\nRel(producer, job, \"Returns data\")\nRel(job, client, \"Updates job status\")</code></pre> <p>You can view and edit this diagram using Mermaid live editors or compatible Markdown viewers.</p> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"deployment/","title":"Deploying a Federator Locally","text":"<p>Repository: <code>federator</code> Description: <code>deploying the federator locally.</code></p>"},{"location":"deployment/#a-guide-showing-the-steps-on-deploying-a-federator-locally","title":"A Guide showing the steps on deploying a federator locally.","text":""},{"location":"deployment/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Prerequisites</li> <li>Certificates &amp; Security</li> <li>Federator Deployment</li> <li>Changing Topics (Optional)</li> </ul>"},{"location":"deployment/#introduction","title":"Introduction","text":"<p>The following guide shows how to deploy a federator locally that can be connected to two IA Nodes.</p> <p>This guide will assume you have already followed the setup in Running the federator locally and have already set up your maven <code>.m2/settings.xml</code> profile and have a working PAT (Personal Access Token).</p>"},{"location":"deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Java 21+, Docker, and Git installed.</li> <li>Management Node (Spring Boot app) running and accessible.</li> <li>Keycloak instance for authentication and JWT issuance.</li> <li>[Postgres] database for Keycloak, typically started via the Management Node Docker Compose file.</li> <li>Valid certificates for mTLS:</li> <li>Keystore in PKCS12 (<code>.p12</code>) format</li> <li>Truststore in JKS (<code>.jks</code>) format</li> <li>All certificates must be trusted by all parties (client, server, Management Node, Keycloak).</li> <li>Docker Compose file for Management Node, Keycloak, and Postgres: Management Node Docker Compose</li> </ul>"},{"location":"deployment/#certificates-security","title":"Certificates &amp; Security","text":"<p>Federator uses mTLS for all secure communication between client, server, Management Node, and Keycloak. You must: - Generate and configure valid certificates for all components. - Reference keystore (.p12) and truststore (.jks) in your properties files. - Certificates are used for mTLS authentication to Keycloak and Management Node, and for producer-consumer communication. - JWT tokens are obtained from Keycloak and used for authorization with Management Node and servers. The server validates JWT tokens and checks the <code>aud</code> claim.</p> <p>For details on generating and configuring certificates, see authentication.md.</p>"},{"location":"deployment/#federator-deployment","title":"Federator Deployment","text":"<ol> <li> <p>Ensure Management Node, Keycloak, and Postgres are running. Use the official Management Node Docker Compose file to spin up these services:    Management Node Docker Compose</p> </li> <li> <p>Locate the <code>docker/docker-compose-grpc.yml</code> file which contains a federator client and server.</p> </li> <li> <p>Replace the <code>image</code> of the <code>federator-server</code> and <code>federator-client</code> with their respective GHCR images, we recommend    checking the released federator packages    for an up-to-date version.</p> </li> </ol> <p><pre><code> federator-server:\n #    image: uk.gov.dbt.ndtp/${ARTIFACT_ID}-server:${VERSION}\n     image: ghcr.io/national-digital-twin/federator/federator-server:0.90.0\n\n... Rest of file ...\n\nfederator-client:\n#  image: uk.gov.dbt.ndtp/${ARTIFACT_ID}-client:${VERSION}\n   image: ghcr.io/national-digital-twin/federator/federator-client:0.90.0\n</code></pre> 4. Start the docker containers with the following command:</p> <p><pre><code>docker compose --file docker/docker-compose-grpc.yml up -d\n</code></pre> 5. Wait for the services to start and data to be loaded into the federated Kafka topic. 6. Check federated messages have been filtered in Kafka with the following Kafka consumer command:</p> <pre><code>./kafka-console-consumer.sh --bootstrap-server localhost:29093 --topic federated-client1-FederatorServer1-knowledge --from-beginning\n</code></pre> <p>Note: Configuration is now dynamic and retrieved from Management Node. Ensure your <code>client.properties</code> and <code>server.properties</code> reference the correct keystore and truststore files, and set the <code>management.node.base.url</code> property.</p> <p>For more details on configuration, see client-configuration.md and server-configuration.md.</p>"},{"location":"deployment/#changing-topics-optional","title":"Changing Topics (Optional)","text":"<p>This sections shows how to add another topic to the federator 'stack' to choose where data is sent and then federated.</p> <ol> <li> <p>Locate the <code>docker/docker-grpc-resources/docker-compose-shared.yml</code> file which contains the source    and target kafka topics, and some helpful shell scripts to add and remove data.</p> </li> <li> <p>On line 135, add <code>RDF</code> to the end:</p> </li> </ol> <pre><code>environment:\n  BOOTSTRAP_VALUE: \"kafka-src:19092\"\n  KAFKA_TOPICS: \"knowledge knowledge1 knowledge2 RDF\"\n</code></pre> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"logging-configuration/","title":"Logging configuration","text":"<p>Repository: <code>federator</code> Description: <code>how to change the logging configuration</code></p>"},{"location":"logging-configuration/#overview","title":"Overview","text":"<p>This describes how to change the logging level of the application.</p>"},{"location":"logging-configuration/#changing-the-level-of-detail-in-the-logging-configuration","title":"Changing the level of detail in the logging configuration","text":"<p>The logging configuration is defined in a properties file <code>logback.xml</code> which is located in the <code>src/main/resources</code> directory.</p> <p>The logging configuration is defined in the <code>&lt;root level=\"Logging-Level\"&gt;</code> element of the file. This element defines the root logger and is the parent of all other loggers in the configuration.</p> <p>For example to set the level of logging to debug, the root element configuration would look like this:</p> <pre><code>&lt;root level=\"DEBUG\"&gt;\n    &lt;appender-ref ref=\"ASYNC_CONSOLE\"/&gt;\n&lt;/root&gt;\n</code></pre> <p>The level attribute can be set to one of the following values: <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, <code>OFF</code></p> <p>Rebuild the application jar and docker images to apply the changes.</p> <p>Once completed you can confirm the changes by running the application and checking the logs (unless you have set them to <code>OFF</code>):</p> <pre><code>11:35:25,960 |-INFO in ch.qos.logback.classic.model.processor.RootLoggerModelHandler - Setting level of ROOT logger to ERROR\n</code></pre> <p>For more information please refer to the logback documentation</p> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"running-locally/","title":"Running the Federator locally","text":"<p>Repository: <code>federator</code> Description: <code>running the federator locally</code></p>"},{"location":"running-locally/#outlines-how-to-install-build-and-run-the-federator-client-and-server-locally","title":"Outlines how to install, build, and run the Federator client and server locally.","text":""},{"location":"running-locally/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Requirements</li> <li>Certificates &amp; Security</li> <li>Configuration</li> <li>Management Node &amp; Keycloak Integration</li> <li>Maven setup</li> <li>Compiling and Running</li> <li>Viewing the Kafka Topics</li> <li>Troubleshooting</li> <li>Further Examples</li> </ul>"},{"location":"running-locally/#introduction","title":"Introduction","text":"<p>The following outlines how to take the java code and run the application locally using resources that are run within Docker containers. These docker containers will start up the Kafka, Zookeeper, and Redis resources.</p> <p>For more information on how to run the docker containers see the docker readme.</p>"},{"location":"running-locally/#requirements","title":"Requirements","text":"<p>This will require: Java (version 21+), Docker, and Git to be installed. Docker desktop is the preferred tool, however a vanilla Docker installation will also work. If you are an Ubuntu user then you might also need to follow some extra steps to get Docker Desktop working see the link here.</p> <p>Additionally, you must have: - Management Node running and accessible. This is a Spring Boot application and must be started for configuration and coordination. - Keycloak instance for authentication and JWT issuance. - [Postgres] database for Keycloak, typically started via the Management Node Docker Compose file.</p>"},{"location":"running-locally/#certificates-security","title":"Certificates &amp; Security","text":"<p>Federator uses mTLS for all secure communication between client, server, Management Node, and Keycloak. You must: - Generate and configure valid certificates for all components (client, server, Management Node, Keycloak). - Use PKCS12 (<code>.p12</code>) format for keystore files. - Use JKS (<code>.jks</code>) format for truststore files. - Ensure the keystore and truststore are correctly referenced in your properties files for both client and server. - Certificates must be trusted by all parties to establish secure connections. - The same certificates are used for mTLS authentication to Keycloak and Management Node, and for producer-consumer communication.</p> <p>Note: If certificates are invalid or not trusted, mTLS connections will fail and configuration/authentication will not work.</p> <p>For details on generating and configuring certificates, see the authentication.md documentation.</p>"},{"location":"running-locally/#configuration","title":"Configuration","text":"<p>Please see the following links for configuration details: - server configuration for more details. - client configuration for more details. - authentication configuration for more details. - logging configuration for more details.</p>"},{"location":"running-locally/#key-properties","title":"Key Properties","text":"<ul> <li>client.properties: Main client configuration. Location defined by the <code>FEDERATOR_CLIENT_PROPERTIES</code> environment variable.</li> <li><code>common.configuration</code>: Path to the common configuration file, set in <code>client.properties</code> (e.g., <code>common.configuration=src/configs/common-configuration.properties</code>).</li> <li><code>management.node.base.url</code>: URL of the Management Node (e.g., <code>https://localhost:8090</code>).</li> <li><code>management.node.request.timeout</code>: Timeout for Management Node requests.</li> <li><code>management.node.cache.ttl.seconds</code>: Cache TTL for Management Node responses.</li> </ul> <p>Note: Properties cannot be overridden by environment variables. All configuration must be set in the properties files.</p>"},{"location":"running-locally/#management-node-keycloak-integration","title":"Management Node &amp; Keycloak Integration","text":"<p>Federator requires Management Node for dynamic configuration and Keycloak for authentication. The client: - Uses mTLS for secure communication with both Management Node and servers. - Obtains a JWT token from Keycloak (using mTLS). - Uses the JWT token to authenticate with Management Node and retrieve configuration (including producer/server details). - The Management Node returns configuration as JSON, which the client uses to connect to servers and manage jobs. - The server validates JWT tokens and checks the <code>aud</code> claim for authorization.</p> <p>To spin up Management Node, Keycloak, and Postgres locally, use the Docker Compose file provided by the Management Node project: Management Node Docker Compose.</p> <p>For more details, see authentication.md.</p>"},{"location":"running-locally/#maven-setup","title":"Maven setup","text":"<p>Currently, we are using GitHub package repositories to distribute libraries. To ensure that Maven have access to the package repositories we need to update the settings. Instructions on how to do this can be found here which will contain additional information.</p> <p>You will need to generate a personal access token that has access to at least reading package registries. Open up <code>$MAVEN_HOME/.m2/settings.xml</code> and add the following</p> <pre><code>&lt;settings&gt;\n    &lt;activeProfiles&gt;\n        &lt;activeProfile&gt;github&lt;/activeProfile&gt;\n    &lt;/activeProfiles&gt;\n\n    &lt;!--  Optional as this will also be defined in the pom file  --&gt;\n    &lt;profiles&gt;\n        &lt;profile&gt;\n            &lt;id&gt;github&lt;/id&gt;\n            &lt;repositories&gt;\n                &lt;repository&gt;\n                    &lt;id&gt;central&lt;/id&gt;\n                    &lt;url&gt;https://repo1.maven.org/maven2&lt;/url&gt;\n                &lt;/repository&gt;\n                &lt;repository&gt;\n                    &lt;id&gt;github&lt;/id&gt;\n                    &lt;url&gt;https://maven.pkg.github.com/National-Digital-Twin/*&lt;/url&gt;\n                    &lt;snapshots&gt;\n                        &lt;enabled&gt;true&lt;/enabled&gt;\n                    &lt;/snapshots&gt;\n                &lt;/repository&gt;\n            &lt;/repositories&gt;\n        &lt;/profile&gt;\n    &lt;/profiles&gt;\n\n    &lt;servers&gt;\n        &lt;server&gt;\n            &lt;id&gt;github&lt;/id&gt;\n            &lt;username&gt;YOUR_GITHUB_USERNAME&lt;/username&gt;\n            &lt;password&gt;YOUR_TOKEN&lt;/password&gt;\n        &lt;/server&gt;\n    &lt;/servers&gt;\n&lt;/settings&gt;\n</code></pre>"},{"location":"running-locally/#compiling-and-running","title":"Compiling and Running","text":""},{"location":"running-locally/#compile-and-build-the-jar-file","title":"Compile and build the JAR file","text":"<ul> <li>From a terminal window navigate to the root directory of the project.  Run the following command to compile the code.</li> </ul> <pre><code>./mvnw clean install\n</code></pre> <p>You should now have built two JAR files: - <code>federator-server-X.X.X.jar</code> (server) - <code>federator-client-0.3.0.jar</code> (client)</p>"},{"location":"running-locally/#running-docker-compose","title":"Running Docker Compose","text":"<p>To start the federators common docker services run the following command:</p> <pre><code>docker compose --file docker/docker-grpc-resources/docker-compose-shared.yml up -d\n</code></pre> <ul> <li>This will create the kafka source <code>kafka-src</code> and target <code>kafka-target</code> together with two zookeeper and one redis containers.</li> <li>It will also create four additional containers to create the test topics and populate them with test data.</li> </ul> <p>To confirm that the containers are running, check this guide:</p>"},{"location":"running-locally/#running-the-federator-server-code","title":"Running the Federator Server Code","text":"<p>Start the federation server code. Note the parameters that are being passed based on properties to tell it the access topics, the Kafka server instance location and port. The remaining properties are picked up from the default properties file bundled into the Jar file.</p> <ul> <li>Export the properties file location to the environment variable <code>FEDERATOR_SERVER_PROPERTIES</code></li> </ul> <pre><code>export FEDERATOR_SERVER_PROPERTIES=./src/configs/server.properties\n</code></pre> <p>This will configure the server to use the properties file server.properties </p> <ul> <li>Start the server code contained in a jar file:</li> </ul> <pre><code>java -jar ./target/federator-server-0.3.0.jar\n</code></pre>"},{"location":"running-locally/#running-the-federator-client-code","title":"Running the Federator Client Code","text":"<ul> <li>Export the properties file location to the environment variable <code>FEDERATOR_CLIENT_PROPERTIES</code></li> </ul> <pre><code>export FEDERATOR_CLIENT_PROPERTIES=./src/configs/client.properties \n</code></pre> <p>This will set the client to use the properties file client.properties and the related common configuration file as defined by the <code>common.configuration</code> property.</p> <ul> <li>Start the client code contained in a jar file:</li> </ul> <pre><code>java -jar ./target/federator-client-0.3.0.jar\n</code></pre>"},{"location":"running-locally/#stopping-the-client-and-server-code","title":"Stopping the Client and Server Code","text":"<ul> <li>To stop either of the client or server code use <code>ctrl c</code> in the terminal window where the code is running.</li> </ul>"},{"location":"running-locally/#stopping-the-docker-containers","title":"Stopping the Docker Containers","text":"<ul> <li>Stop the docker containers with the commands in the docker docs</li> </ul>"},{"location":"running-locally/#viewing-the-kafka-topics","title":"Viewing the Kafka Topics","text":""},{"location":"running-locally/#viewing-the-kafka-topics-on-the-target-kafka-instance","title":"Viewing the Kafka Topics on the Target Kafka Instance","text":"<ul> <li>To view the processed Kafka topics on the target Kafka instance use the following command:</li> </ul> <pre><code>docker exec -it kafka-target kafka-console-consumer --bootstrap-server localhost:29093 --topic federated-Localhost-createDataTopic1 --from-beginning --property print.headers=true\n</code></pre> <ul> <li>This will show the messages that have been processed by the federator client code.</li> <li>The topic name will be prefixed with <code>federated-</code> and the original topic name (<code>federated-Localhost-createDataTopic1</code>).</li> </ul>"},{"location":"running-locally/#viewing-the-kafka-topics-on-the-source-kafka-instance","title":"Viewing the Kafka Topics on the Source Kafka Instance","text":"<ul> <li>To view the Kafka topics on the source Kafka instance use the following command:</li> </ul> <pre><code>docker exec -it kafka-src kafka-console-consumer --bootstrap-server localhost:19093 --topic createDataTopic1 --from-beginning --property print.headers=true\n</code></pre> <ul> <li>This will show the test messages that have been loaded into the source Kafka instance.</li> </ul>"},{"location":"running-locally/#smoke-tests","title":"Smoke Tests","text":"<ul> <li>To run smoke tests, you will need to set up the compose files in this guide</li> </ul>"},{"location":"running-locally/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Ensure Management Node, Keycloak, and Postgres are running and accessible from Federator.</li> <li>Check mTLS certificates, keystore, and truststore configuration for all components.</li> <li>Verify that the <code>common.configuration</code> property in <code>client.properties</code> points to the correct file.</li> <li>Properties must be set in the properties files and cannot be overridden by environment variables.</li> <li>If you encounter issues with authentication or configuration retrieval, check certificate validity and trust relationships.</li> </ul>"},{"location":"running-locally/#further-examples","title":"Further Examples","text":"<p>Further example configurations can be found in the configs readme.</p> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"server-configuration/","title":"Server Configuration","text":"<p>Repository: <code>federator</code> Description: <code>server configuration</code></p>"},{"location":"server-configuration/#overview","title":"Overview","text":"<p>This document describes the configuration properties for the Federator server, including connection settings, filtering, and common configuration options shared across Federator components.</p>"},{"location":"server-configuration/#configuration-files","title":"Configuration Files","text":"<ul> <li>server.properties: Main server configuration. Location defined by the <code>FEDERATOR_SERVER_PROPERTIES</code> environment variable.</li> <li>common-configuration.properties: Contains properties shared by both server and client. Location defined by the <code>FEDERATOR_COMMON_PROPERTIES</code> environment variable.</li> </ul>"},{"location":"server-configuration/#server-properties","title":"Server Properties","text":"Property Description <code>kafka.bootstrapServers</code> Bootstrap servers for the source Kafka instance <code>kafka.defaultKeyDeserializerClass</code> Default key deserializer class <code>kafka.defaultValueDeserializerClass</code> Default value deserializer class <code>kafka.consumerGroup</code> Consumer group for the server <code>kafka.pollDuration</code> Duration to poll for messages (ms) <code>kafka.pollRecords</code> Number of records to poll per request <code>kafka.additional.*</code> Optional: Additional Kafka consumer properties (prefix removed before passing to Kafka) <code>shared.headers</code> Definition of Kafka message headers <code>filter.shareAll</code> If true, all messages are shared (no filtering) <code>server.port</code> Port the server listens on <code>server.tlsEnabled</code> If true, enables TLS for gRPC communication <code>server.keepAliveTime</code> Keep-alive time for the server (ms) <code>consumer.inactivity.timeout</code> Duration of inactivity (ISO-8601, e.g. PT30S) before the server disconnects the consumer <p>Note: - The <code>consumer.inactivity.timeout</code> property controls how long the server will wait for messages before disconnecting a consumer due to inactivity. If no messages are received within this duration, the consumer connection is closed automatically. This helps free up resources and ensures efficient operation.</p>"},{"location":"server-configuration/#common-configuration-properties","title":"Common Configuration Properties","text":"<p>Common properties are defined in <code>common-configuration.properties</code> and may include:</p> Property Description <code>idp.mtls.enabled</code> Enable mutual TLS for Identity Provider (IDP) communication (<code>true</code>/<code>false</code>) <code>idp.client.secret</code> OAuth2 client secret (used when mTLS is disabled) <code>idp.jwks.url</code> JWKS URL of the IDP (used for verifying JWT signatures) <code>idp.token.url</code> Token endpoint URL of the IDP (used to fetch OAuth2 tokens) <code>idp.token.backoff</code> Backoff time in milliseconds before retrying a failed token request (default: 1000 ms) <code>idp.client.id</code> OAuth2 client ID (registered with the IDP) <code>idp.keystore.path</code> Path to client keystore file (PKCS12 or JKS) for mutual TLS <code>idp.keystore.password</code> Password for the client keystore <code>idp.truststore.path</code> Path to truststore containing the IDP's root CA or certificate chain <code>idp.truststore.password</code> Password for the truststore ... Other shared properties as required <p>All properties can be set directly or via environment variables (see <code>${...}</code> syntax in the file). These settings ensure secure authentication and authorisation between Federator components and the Identity Provider, supporting both mTLS and OAuth2 flows.</p>"},{"location":"server-configuration/#custom-filter-configuration","title":"Custom Filter Configuration","text":"<p>Federator supports custom filtering logic for Kafka messages. The producer (server) receives consumer attribute configuration from the Management-Node as JSON, for example:</p> <pre><code>{\n  \"clientId\": \"FEDERATOR_HEG\",\n  \"producers\": [\n    {\n      \"products\": [\n        {\n          \"name\": \"BrownfieldLandAvailability\",\n          \"topic\": \"topic.BrownfieldLandAvailability\",\n          \"consumers\": [\n            {\n              \"name\": \"ENV-CONSUMER-1\",\n              \"idpClientId\": \"FEDERATOR_ENV\",\n              \"attributes\": [\n                { \"name\": \"nationality\", \"value\": \"GBR\", \"type\": \"string\" },\n                { \"name\": \"clearance\", \"value\": \"0\", \"type\": \"string\" },\n                { \"name\": \"organisation_type\", \"value\": \"NON-GOV3\", \"type\": \"string\" }\n              ]\n            }\n          ]\n        }\n      ],\n      \"name\": \"HEG-PRODUCER-1\",\n      \"idpClientId\": \"FEDERATOR_HEG\"\n    }\n  ]\n}\n</code></pre>"},{"location":"server-configuration/#filtering-logic","title":"Filtering Logic","text":"<ul> <li>No attributes set: All data is passed to the client.</li> <li>One attribute set: Only messages with a matching attribute and value are passed.</li> <li>Multiple attributes set: Messages must match all attributes (AND logic).</li> </ul> <p>The filter checks Kafka message headers for the required attributes and values. For example, if a consumer requires <code>nationality=GBR</code> and <code>clearance=0</code>, only messages with both headers will be delivered.</p>"},{"location":"server-configuration/#example-filter-implementation","title":"Example Filter Implementation","text":"<pre><code>public class AttributeBasedMessageFilter implements MessageFilter {\n    private final Map&lt;String, String&gt; requiredAttributes;\n    public AttributeBasedMessageFilter(Map&lt;String, String&gt; requiredAttributes) {\n        this.requiredAttributes = requiredAttributes;\n    }\n    @Override\n    public boolean filterOut(KafkaEvent&lt;?, ?&gt; message) {\n        if (requiredAttributes == null || requiredAttributes.isEmpty()) {\n            return false; // No attributes: pass all messages\n        }\n        for (Map.Entry&lt;String, String&gt; entry : requiredAttributes.entrySet()) {\n            String headerValue = message.getHeader(entry.getKey());\n            if (!entry.getValue().equals(headerValue)) {\n                return true; // Filter out if any attribute does not match\n            }\n        }\n        return false; // Pass if all attributes match\n    }\n}\n</code></pre> <p>For details on authentication and key generation, see Authentication Configuration.</p>"},{"location":"server-configuration/#interceptors","title":"Interceptors","text":"<p>Federator uses gRPC server interceptors to enforce authentication and authorization for all incoming requests:</p>"},{"location":"server-configuration/#authserverinterceptor","title":"AuthServerInterceptor","text":"<ul> <li>Validates the presence and format of the Authorization header (expects a Bearer token).</li> <li>Verifies the JWT token using the Identity Provider (IDP) public keys or mTLS.</li> <li>Rejects requests with missing, invalid, or expired tokens.</li> <li>Configuration properties used: <code>idp.jwks.url</code>, <code>idp.token.url</code>, <code>idp.mtls.enabled</code>, etc.</li> </ul>"},{"location":"server-configuration/#consumerverificationserverinterceptor","title":"ConsumerVerificationServerInterceptor","text":"<ul> <li>Extracts the consumer ID from the JWT token.</li> <li>Validates the token audience against the configured client ID (<code>idp.client.id</code>).</li> <li>Authorizes the consumer by checking the producer configuration (from Management-Node) to ensure the consumer is permitted for the requested product/topic.</li> <li>Attaches the verified consumer ID to the gRPC context for downstream use.</li> <li>Rejects requests from unauthorized consumers or with invalid audience.</li> </ul> <p>These interceptors are essential for secure, multi-tenant operation, ensuring only authenticated and authorized clients can access federated data. For implementation details, see: - <code>AuthServerInterceptor.java</code> - <code>ConsumerVerificationServerInterceptor.java</code></p>"},{"location":"server-configuration/#grpc-request-sequence-diagram","title":"gRPC Request Sequence Diagram","text":"<pre><code>sequenceDiagram\n    participant Client as gRPC Client\n    participant Auth as AuthServerInterceptor\n    participant Verify as ConsumerVerificationServerInterceptor\n    participant Server as Federator Server\n    participant Mgmt as Management-Node\n    participant Kafka as Kafka/Redis\n    participant Filter as Custom Filter\n\n    Client-&gt;&gt;Auth: Send gRPC request with Bearer token\n    Auth-&gt;&gt;Auth: Validate Authorization header &amp; JWT\n    Auth--&gt;&gt;Client: Reject if invalid\n    Auth-&gt;&gt;Verify: Pass request if valid\n    Verify-&gt;&gt;Verify: Extract consumer ID from JWT\n    Verify-&gt;&gt;Verify: Validate audience (idp.client.id)\n    Verify-&gt;&gt;Mgmt: Fetch producer/consumer config\n    Verify-&gt;&gt;Verify: Authorize consumer for topic\n    Verify--&gt;&gt;Client: Reject if unauthorized\n    Verify-&gt;&gt;Server: Pass request if authorized\n    Server-&gt;&gt;Kafka: Read messages from Kafka\n    Server-&gt;&gt;Filter: Apply custom filter (attributes)\n    Filter--&gt;&gt;Server: Return filtered messages\n    Server--&gt;&gt;Client: Send federated data</code></pre> <p>Maintained by the National Digital Twin Programme (NDTP).</p> <p>\u00a9 Crown Copyright 2025. This work has been developed by the National Digital Twin Programme and is legally attributed to the Department for Business and Trade (UK) as the governing entity. Licensed under the Open Government Licence v3.0. For full licensing terms, see OGL_LICENSE.md.</p>"},{"location":"smoke-tests/one-to-one/","title":"One to One (Single Client with Single Server) Smoke Tests","text":""},{"location":"smoke-tests/one-to-one/#use-case","title":"Use Case","text":"<p>Given I send the <code>simple-sample-test.dat</code> data file to producer Kafka</p> <p>When that file contains 40 records of test data and the following filters are applied:</p> <ul> <li>Nationality: <code>GBR</code></li> <li>Clearance: <code>O</code></li> <li>Organisation Type: <code>NON-GOV</code></li> </ul> <p>Then I should see 23 records in the consumer IA Node data store, and those records should contain the same attributes listed above (Nationality, Clearance, Organisation Type)</p>"},{"location":"smoke-tests/one-to-one/#running-the-tests","title":"Running the tests","text":"<p>To run the tests you'll need to spin up the following docker compose file:</p> <pre><code>docker compose --file docker/docker-compose-grpc.yml up -d   \n</code></pre>"},{"location":"smoke-tests/one-to-one/#changing-the-test-parameters","title":"Changing the test parameters","text":""},{"location":"smoke-tests/one-to-one/#changing-the-test-data-file","title":"Changing the test data file","text":"<p>To change the test data file that is sent via kafka, you must modify the <code>KNOWLEDGE_DATA</code> environment variable in the <code>docker/.env</code> to the filename of your desired file, ensuring that the file is in the <code>docker/input/knowledge</code> directory.</p> <pre><code>KNOWLEDGE_DATA=simple-sample-test.dat  # Change this to your desired filename\n</code></pre>"},{"location":"smoke-tests/one-to-one/#changing-the-filters","title":"Changing the filters","text":"<p>Changing the data filters is now managed through the management node.</p>"},{"location":"smoke-tests/one-to-one/#viewing-the-test-outcome","title":"Viewing the test outcome","text":"<p>The test outcome can be viewed by checking the logs of kafka-message-counter, which will check the message count in the Kafka topic against the expected number of messages:</p> <pre><code>docker logs kafka-message-counter --follow \n</code></pre> <p>There may be cases where the message counter needs more time to pick up the federated messages. To resolve any test failures related to this, you can restart the <code>kafka-message-counter</code> container</p> <pre><code>docker restart kafka-message-counter\n</code></pre> <p>and then rerun the logs command above.</p>"},{"location":"smoke-tests/one-to-one/#clean-up","title":"Clean up","text":"<p>When finished with the smoke tests you can take down the containers to save resources:</p> <pre><code>docker compose --file docker/docker-compose-grpc.yml down\n</code></pre>"}]}